# SQL for Data Analysis: The Basics

This guide explains the foundational concepts of SQL for data analysis, specifically tailored for those who are already familiar with Python and Pandas. While we focus on **PostgreSQL**, we will highlight major differences with other SQL dialects (like MySQL or SQLite) where relevant.

---

## 1. Introduction to Relational Databases and SQL

### What is SQL?
**SQL** (Structured Query Language) is the standard language for managing and manipulating relational databases. Unlike imperative languages like Python (where you tell the computer *how* to do something step-by-step), SQL is **declarative**—you describe *what* result you want, and the database engine figures out the most efficient way to get it.

### What does "Relational" mean?
A **Relational Database** organizes data into **tables** (also called relations) which can be linked—or *related*—to each other based on data common to each. This structure minimizes redundancy and improves data integrity.
- **Tables**: Grids of rows and columns (similar to Excel sheets or Pandas DataFrames).
- **Relationships**: Logical connections between tables (e.g., an `orders` table linking to a `customers` table via a `customer_id`).

### Why use SQL over Excel or CSVs?
While flat files (CSV, Excel) are great for small, simple datasets, Relational Databases (RDBMS) are necessary when:
- **Scale**: You have millions of rows that don't fit in memory (RAM).
- **Integrity**: You need strict rules (e.g., "Email must be unique", "Age cannot be negative").
- **Concurrency**: Multiple users need to read/write data at the same time safely.
- **Security**: You need granular control over who can see or edit specific data.

In your previous work with Pandas, you handled data using **DataFrames** stored in memory (RAM). A **Relational Database (RDBMS)** like PostgreSQL stores data on disk in a structured format called **Tables**.

### Key Differences: Pandas vs. SQL
| Feature | Pandas | SQL (PostgreSQL) |
| :--- | :--- | :--- |
| **Storage** | In-memory (RAM) | On-disk (Persistent) |
| **Data Unit** | DataFrame | Table |
| **Row Identifier** | Index (Implicit or Explicit) | Primary Key (Explicit) |
| **Execution** | Imperative (How to do it) | Declarative (What to get) |

### Comments in SQL
Just like `#` in Python, SQL uses specific syntax for comments:
- **Single-line**: `-- comment here`
- **Multi-line**: `/* comment here */`

---

### Data Types and Constraints

In Pandas, you use `dtypes` (int64, float64, object). In SQL, you must define the data type for every column when a table is created.

### Common PostgreSQL Data Types
| PostgreSQL Type | Pandas Equivalent | Description |
| :--- | :--- | :--- |
| `INT` or `INTEGER` | `int64` | Whole numbers. |
| `NUMERIC(p, s)` | `float64` | Precise decimals (used for money). |
| `TEXT` or `VARCHAR(n)`| `object` / `string` | Variable-length strings. |
| `BOOLEAN` | `bool` | True or False. |
| `DATE` / `TIMESTAMP` | `datetime64` | Dates and times. |

> **Note on Dialects**: Some databases (like MySQL) use `DATETIME` instead of `TIMESTAMP`. SQLite is much simpler and often stores most things as `TEXT` or `INTEGER`.

### Constraints
Constraints are rules applied to columns to ensure data integrity. This is something Pandas doesn't strictly enforce by default.
- **`NOT NULL`**: Ensures a column cannot have a `NULL` (NaN) value.
- **`UNIQUE`**: Ensures all values in a column are different.
- **`PRIMARY KEY`**: A combination of `NOT NULL` and `UNIQUE`. Uniquely identifies each row.
- **`FOREIGN KEY`**: Ensures a value matches a value in another table (used for joining).
- **`CHECK`**: Ensures values meet a specific condition (e.g., `price > 0`).

---

## 2. Querying and Selecting Data

Querying in SQL is the equivalent of selecting columns and rows in Pandas.

### Selecting All Columns (`SELECT *`)
In Pandas, if you want to view the entire DataFrame, you simply reference the variable `df`. In SQL, you use the asterisk (`*`) wildcard to select every column from a table.

**Pandas:**
```python
df
```

**SQL:**
```sql
SELECT * 
FROM users;
```

### Selecting Specific Columns
In Pandas, you might select a subset of columns using a list: `df[['name', 'age']]`. In SQL, you list the column names directly after `SELECT`.

**Pandas:**
```python
df[['name', 'age']]
```

**SQL:**
```sql
SELECT name, age
FROM users;
```

### Aliases (`AS`)
Aliases allow you to rename a column or table in the output, similar to `.rename(columns={'old': 'new'})` in Pandas.

```sql
SELECT user_name AS name, user_age AS age
FROM users;
```

### `DISTINCT` for Unique Values
If you want to find unique values in a column, Pandas uses `.unique()` or `.drop_duplicates()`. In SQL, you use the `DISTINCT` keyword.

```sql
-- Returns unique cities from the users table
SELECT DISTINCT city
FROM users;
```

When you use `DISTINCT` with **multiple columns**, it returns unique **rows** based on the combination of those columns.

```sql
-- Returns unique combinations of city and country
SELECT DISTINCT city, country
FROM users;
```

**Pandas Equivalent:**
```python
# Drop duplicates based on multiple columns
df[['city', 'country']].drop_duplicates()
```

> **Dialect Tip**: While `DISTINCT` is standard across all SQL versions, some databases like PostgreSQL offer `DISTINCT ON (column)`.

---

## 3. Filtering and Sorting Data

### The Basic Structure with Filtering: `SELECT`, `FROM`, `WHERE`
When you combine column selection with row filtering, the order in SQL is always fixed.

1.  **`SELECT`**: Defines which columns to return (similar to `df[[cols]]`). Use `SELECT *` to select all columns (like `df`).
2.  **`FROM`**: Defines the source table.
3.  **`WHERE`**: Filters rows based on a condition (similar to boolean indexing).

These operations are the bread and butter of data analysis, equivalent to boolean indexing and `.sort_values()` in Pandas.

### Filtering Rows with `WHERE`
In Pandas, you filter rows using boolean indexing: `df[df['age'] > 25]`. In SQL, you use the `WHERE` clause.

**Pandas:**
```python
# Filtering rows where age is greater than 25
df[df['age'] > 25]
```

**SQL:**
```sql
SELECT *
FROM users
WHERE age > 25;
```

### Comparison Operators: SQL vs Pandas

| Operator Type | SQL | Pandas |
| :--- | :--- | :--- |
| **Equality** | `=` | `==` |
| **Inequality** | `<>` or `!=` | `!=` |
| **Greater Than** | `>` | `>` |
| **Less Than** | `<` | `<` |
| **Greater/Equal** | `>=` | `>=` |
| **Less/Equal** | `<=` | `<=` |

#### Filtering with Strings
When filtering strings in SQL, you must use **single quotes** (`'`). In Python, you can use either single or double quotes.

**Pandas:**
```python
# Strings can be in ' ' or " "
df[df['city'] == "New York"]
```

**SQL:**
```sql
-- Single quotes ONLY for string literals
SELECT *
FROM users
WHERE city = 'New York';
```

> **Crucial Syntax Differences**:
> 1. **Quotes**: In SQL, **Single Quotes** (`'New York'`) are for string values. **Double Quotes** (`"column_name"`) are used for identifiers (like table or column names with spaces or reserved words). In Python, both work for strings.
> 2. **Equality**: SQL uses a **Single Equal Sign** (`=`) for comparison. Python uses a **Double Equal Sign** (`==`) for comparison (since `=` is for assignment).
> 3. **Case Sensitivity**: In PostgreSQL, string comparisons are **case-sensitive** by default. `'New York'` is not the same as `'new york'`.

### Combining Selection and Filtering
You can select specific columns and filter rows simultaneously.

**Pandas:**
```python
# Filter rows first, then select columns
df[df['age'] > 25][['name', 'city']]
```

**SQL:**
```sql
SELECT name, city
FROM users
WHERE age > 25;
```

> **Important Note on Execution Order**: In SQL, even though we write `SELECT` first, the database actually executes `FROM` first, then `WHERE`, and finally `SELECT`. This is why the Pandas equivalent `df[df['age'] > 25][['name', 'city']]` is more accurate—it filters the rows before picking the columns.

### Sorting with `ORDER BY`
In Pandas, you use `df.sort_values(by='col', ascending=True)`. In SQL, you use `ORDER BY`.

```sql
SELECT name, age
FROM users
ORDER BY age DESC, name ASC;
```
- **`ASC`**: Ascending (default).
- **`DESC`**: Descending.

### Combining `SELECT`, `FROM`, `WHERE`, and `ORDER BY`
When combining all these clauses, the order in SQL must be: `SELECT` → `FROM` → `WHERE` → `ORDER BY`.

**Pandas:**
```python
# Filter rows first, then select columns, then sort
df[df['age'] > 25][['name', 'age']].sort_values(by='age', ascending=False)
```

**SQL:**
```sql
SELECT name, age
FROM users
WHERE age > 25
ORDER BY age DESC;
```

> **Note on Logical Order**: Again, the database executes this as: `FROM` → `WHERE` → `SELECT` → `ORDER BY`. This is why you can sort by columns that you have selected, but the filtering happens before the sorting.

### PostgreSQL Special: `DISTINCT ON`
`DISTINCT ON` is a powerful PostgreSQL feature that keeps only the "first" row for each unique value in a specific column. This is equivalent to `df.drop_duplicates(subset=['col'], keep='first')` in Pandas and is commonly used to find "the latest X for each Y".

```sql
-- Returns the most recent order for each customer
SELECT DISTINCT ON (customer_id) customer_id, order_date, total_amount
FROM orders
ORDER BY customer_id, order_date DESC;
```

*Note: You must `ORDER BY` the column(s) in `DISTINCT ON` first to ensure deterministic results.*

### Limiting Results with `LIMIT`
Similar to `df.head(10)`, SQL uses `LIMIT`.

```sql
SELECT * FROM users
LIMIT 10;
```

> **Note**: `LIMIT` is always the last clause in a query.

### Logical and Comparison Operators
SQL uses readable words instead of symbols for logic.

| Operation | SQL | Pandas |
| :--- | :--- | :--- |
| **AND** | `WHERE cond1 AND cond2` | `df[(cond1) & (cond2)]` |
| **OR** | `WHERE cond1 OR cond2` | `df[(cond1) \| (cond2)]` |
| **NOT** | `WHERE NOT cond` | `df[~(cond)]` |
| **In List** | `WHERE col IN (1, 2, 3)` | `df[df['col'].isin([1, 2, 3])]` |
| **Range** | `WHERE col BETWEEN 10 AND 20`| `df[df['col'].between(10, 20)]` |

### Pattern Matching with `LIKE`
To find strings that match a pattern (similar to `.str.contains()` or `.str.startswith()`), use `LIKE` or `ILIKE` (PostgreSQL's case-insensitive version).

- `%` matches any sequence of characters.
- `_` matches any single character.

```sql
-- Find names starting with 'A'
SELECT * FROM users WHERE name LIKE 'A%';

-- Find names containing 'smith' (case-insensitive in Postgres)
SELECT * FROM users WHERE name ILIKE '%smith%';
```

> **Note**: in `pandas` you can use `.str.contains(case=False)` for case-insensitive pattern matching.

### Handling Nulls
In SQL, `NULL` represents missing data (NaN). You cannot use `=` to check for `NULL`.

| Feature | SQL | Pandas |
| :--- | :--- | :--- |
| **Is Null?** | `WHERE col IS NULL` | `df[df['col'].isna()]` or `df[df['col'].isnull()]` |
| **Is Not Null?** | `WHERE col IS NOT NULL` | `df[df['col'].notna()]` or `df[df['col'].notnull()]` |
| **Default Value**| `COALESCE(col, 0)` | `df['col'].fillna(0)` |
| **Conditional Null**| `NULLIF(col, 0)` | `df['col'].replace(0, np.nan)` |

> **PostgreSQL Pro-Tip**: `COALESCE` is extremely useful because it returns the first non-null value in a list of arguments. `NULLIF` is often used to avoid "division by zero" errors by turning a `0` into a `NULL`.

## 4. Data Manipulation and Cleaning

This section covers how to transform data, similar to using `.apply()`, `.astype()`, and `.str` methods in Pandas.

### Conditional Logic with `CASE`
The `CASE` statement is SQL's way of doing `if-else` logic. In Pandas, this is equivalent to `np.where()` or applying a function with `if-else`.

**Pandas:**
```python
# Option 1: Using np.where (Vectorized and efficient)
df['status'] = np.where(df['age'] >= 18, 'Adult', 'Minor')

# Option 2: Using .apply (More flexible but slower)
df['status'] = df['age'].apply(lambda x: 'Adult' if x >= 18 else 'Minor')

# Option 3: Using .mask (Updates values where condition is True)
df['status'] = 'Minor'
df['status'] = df['status'].mask(df['age'] >= 18, 'Adult')
```

**SQL:**
```sql
SELECT name,
       CASE 
           WHEN age >= 18 THEN 'Adult'
           ELSE 'Minor'
       END AS status
FROM users;
```

### Type Conversion with `CAST`
Changing data types is done with `CAST` or the PostgreSQL shorthand `::`. This is equivalent to `.astype()` in Pandas.

**Pandas:**
```python
df['id_str'] = df['id'].astype(str)
```

**SQL:**
```sql
SELECT CAST(id AS TEXT) AS id_str,
       id::TEXT AS id_str_shorthand -- Postgres specific
FROM users;
```

#### Why use `CAST`?
In SQL, data types are strict. You will often need `CAST` for:
1.  **Mathematical Operations**: You cannot multiply a `TEXT` column by a number, even if it contains numeric characters. You must cast it to `NUMERIC` or `INT` first.
2.  **Matching Types for Joins**: If you try to join two tables on columns with different types (e.g., an `INT` ID and a `TEXT` ID), the query will error out.
3.  **Correcting Data Entry**: Sometimes numbers or dates are stored as strings. Casting them allows you to use date-specific or numeric-specific functions.
4.  **Formatting**: Converting numbers to strings so you can concatenate them with other text (e.g., `CONCAT('ID: ', id::TEXT)`).

---

### String Functions
Pandas uses `.str` accessor for these operations.

| Operation | SQL | Pandas |
| :--- | :--- | :--- |
| **Concatenate** | `CONCAT(first, ' ', last)` | `df['first'] + ' ' + df['last']` |
| **Uppercase** | `UPPER(col)` | `df['col'].str.upper()` |
| **Lowercase** | `LOWER(col)` | `df['col'].str.lower()` |
| **Trim Spaces** | `TRIM(col)` | `df['col'].str.strip()` |
| **Replace** | `REPLACE(col, 'old', 'new')`| `df['col'].str.replace('old', 'new')` |
| **Length** | `LENGTH(col)` | `df['col'].str.len()` |
| **Substring** | `SUBSTRING(col FROM 1 FOR 3)`| `df['col'].str[0:3]` |
| **Left/Right** | `LEFT(col, 3)` / `RIGHT(col, 3)`| `df['col'].str[:3]` / `df['col'].str[-3:]` |

---

### Numerical Functions
Most of these match standard mathematical functions in Python's `math` or `numpy` libraries.

| Operation | SQL | Pandas / Numpy |
| :--- | :--- | :--- |
| **Absolute** | `ABS(col)` | `df['col'].abs()` |
| **Ceiling** | `CEIL(col)` | `np.ceil(df['col'])` |
| **Floor** | `FLOOR(col)` | `np.floor(df['col'])` |
| **Rounding** | `ROUND(col, 2)` | `df['col'].round(2)` |
| **Square Root** | `SQRT(col)` | `np.sqrt(df['col'])` |
| **Power** | `POWER(col, 2)` | `df['col'] ** 2` |

---

### Date Functions
Working with dates is a critical part of data analysis.

| Operation | SQL (PostgreSQL) | Pandas |
| :--- | :--- | :--- |
| **Extract Part** | `EXTRACT(YEAR FROM col)` | `df['col'].dt.year` |
| **Date Trunc** | `DATE_TRUNC('month', col)` | `df['col'].dt.to_period('M')` |
| **Difference** | `col2 - col1` or `AGE(c2, c1)` | `df['col2'] - df['col1']` |
| **Current Timestamp** | `NOW()` / `CURRENT_TIMESTAMP` | `pd.Timestamp.now()` |
| **Add/Sub** | `col + INTERVAL '1 day'` | `df['col'] + pd.Timedelta(days=1)` |

**Casting and literals for dates/times:**
- Convert text to dates/timestamps: `TO_DATE('2024-01-31','YYYY-MM-DD')`, `TO_TIMESTAMP('2024-01-31 14:30','YYYY-MM-DD HH24:MI')`, or `CAST(col AS DATE/TIMESTAMP)`.
- Date/time constants: `DATE '2024-01-31'`, `TIME '14:30'`, `TIMESTAMP '2024-01-31 14:30'`, `INTERVAL '15 minutes'`.

> **PostgreSQL Tip**: The `AGE()` function is unique to Postgres and returns a human-readable interval (e.g., "2 years 3 months"). For simple day differences, you can just subtract two dates.

---

## 5. Combining Tables (Joins and Set Operations)

In data analysis, you rarely find all the data you need in a single table. You often need to combine multiple tables based on related columns. In Pandas, you use `pd.merge()` and `pd.concat()`. In SQL, you use **Joins** and **Set Operations**.

### Keys: The Foundation of Relationships
To link tables, you need columns that act as identifiers:
- **Primary Key (PK)**: A column (or set of columns) that uniquely identifies each row in a table.
- **Foreign Key (FK)**: A column in one table that refers to the Primary Key of another table.

### Joins: Horizontal Combinations
Joins combine columns from two tables based on a related column (the key). This is equivalent to `pd.merge()`.

| SQL Join | Pandas Equivalent | Description |
| :--- | :--- | :--- |
| `INNER JOIN` | `pd.merge(how='inner')` | Returns only rows with matching values in both tables. |
| `LEFT JOIN` | `pd.merge(how='left')` | Returns all rows from the left table, and matches from the right. |
| `RIGHT JOIN` | `pd.merge(how='right')` | Returns all rows from the right table, and matches from the left. |
| `FULL OUTER JOIN` | `pd.merge(how='outer')` | Returns all rows when there is a match in either table. |

**SQL Example:**
```sql
SELECT orders.order_id, users.name
FROM orders
INNER JOIN users ON orders.user_id = users.user_id;
```

**Pandas Equivalent:**
```python
pd.merge(orders, users, on='user_id', how='inner')
```

#### Self Joins and Cross Joins
- **Self Join**: A table joined with itself. Useful for hierarchical data (e.g., a `users` table where one column is `manager_id` referring to another `user_id`).
- **Cross Join**: Returns the Cartesian product of both tables (every row from table A paired with every row from table B). Equivalent to `pd.merge(how='cross')`.

```sql
-- Self Join Example
SELECT u.name AS user, m.name AS manager
FROM users u
JOIN users m ON u.manager_id = m.user_id;

-- Cross Join Example
SELECT products.name, colors.name
FROM products
CROSS JOIN colors;
```

#### SQL Join Shortcuts and Defaults
In professional SQL code, you will often see shortcuts used to make queries more concise.

1.  **`JOIN` defaults to `INNER JOIN`**: If you just write `JOIN`, the database assumes you mean `INNER JOIN`.
    ```sql
    -- These are identical
    SELECT * FROM orders JOIN users ON orders.id = users.id;
    SELECT * FROM orders INNER JOIN users ON orders.id = users.id;
    ```
2.  **The `USING` Clause**: If the joining columns have the **exact same name** in both tables, you can use `USING` instead of `ON`.
    ```sql
    -- Shortcut
    SELECT * FROM orders JOIN users USING (user_id);

    -- Standard
    SELECT * FROM orders JOIN users ON orders.user_id = users.user_id;
    ```
3.  **Table Aliases**: As seen in the Self Join example, you should always use short aliases when joining multiple tables to keep the code readable.
    ```sql
    SELECT o.order_date, u.name
    FROM orders o
    JOIN users u ON o.user_id = u.user_id;
    ```

---

### Set Operations: Vertical Combinations
Set operations combine the **results** of two queries vertically. This is similar to `pd.concat()`. For these to work, both queries must have the same number of columns and compatible data types.

| SQL Operation | Pandas Equivalent | Description |
| :--- | :--- | :--- |
| `UNION` | `pd.concat().drop_duplicates()` | Combines results and removes duplicates. |
| `UNION ALL` | `pd.concat()` | Combines results and keeps all duplicates (faster). |
| `INTERSECT` | `pd.merge(how='inner')` | Returns only rows that appear in both results. |
| `EXCEPT` | `df1[~df1.isin(df2)]` | Returns rows in the first result but not the second. |

**SQL Example:**
```sql
-- Combine two lists of emails, removing duplicates
SELECT email FROM customers
UNION
SELECT email FROM leads;
```

**Pandas Equivalent:**
```python
pd.concat([customers['email'], leads['email']]).drop_duplicates()
```

**`INTERSECT` Example:**
Returns emails that are in both the `customers` and `leads` tables.
```sql
SELECT email FROM customers
INTERSECT
SELECT email FROM leads;
```

**`EXCEPT` Example:**
Returns emails that are in `customers` but **not** in `leads`.
```sql
SELECT email FROM customers
EXCEPT
SELECT email FROM leads;
```

> **Note on Performance**: Use `UNION ALL` instead of `UNION` if you don't care about duplicates. `UNION` requires the database to perform a sort and de-duplicate step, which is slower on large datasets.

---

## 6. Aggregation and Grouping

Grouping and aggregating data is one of the most powerful features of SQL, directly equivalent to `df.groupby()` in Pandas.

### Aggregate Functions
These functions take multiple rows and return a single value.

| SQL Function | Pandas Equivalent | Description |
| :--- | :--- | :--- |
| `COUNT(*)` | `df.shape[0]` or `len(df)` | Total number of rows. |
| `COUNT(col)` | `df['col'].count()` | Number of non-null values. |
| `SUM(col)` | `df['col'].sum()` | Sum of values. |
| `AVG(col)` | `df['col'].mean()` | Average of values. |
| `MIN(col)` | `df['col'].min()` | Minimum value. |
| `MAX(col)` | `df['col'].max()` | Maximum value. |

### Grouping with `GROUP BY`
In Pandas, you use `df.groupby('col').agg()`. In SQL, any column in the `SELECT` list that isn't an aggregate function **must** be in the `GROUP BY` clause.

**Pandas:**
```python
df.groupby('city')['age'].mean()
```

**SQL:**
```sql
SELECT city, AVG(age)
FROM users
GROUP BY city;
```

### Combining `WHERE` and `GROUP BY`
When you need to filter rows *before* grouping them, you use `WHERE` followed by `GROUP BY`.

**Pandas:**
```python
# Filter individual rows first, then group
df[df['age'] >= 18].groupby('city')['age'].mean()
```

**SQL:**
```sql
SELECT city, AVG(age)
FROM users
WHERE age >= 18
GROUP BY city;
```

> **Execution Order Note**: The database executes this as: `FROM` → `WHERE` → `GROUP BY` → `SELECT`. It first grabs the data, filters the rows, groups the remaining rows, and finally calculates the averages for the display.

### Filtering Groups with `HAVING`
In Pandas, you might filter groups after aggregation using boolean indexing on the result. In SQL, you cannot use `WHERE` to filter on aggregate results; you must use `HAVING`.

**Pandas:**
```python
grouped = df.groupby('city')['age'].mean()
result = grouped[grouped > 30]
```

**SQL:**
```sql
SELECT city, AVG(age)
FROM users
GROUP BY city
HAVING AVG(age) > 30;
```

> **The Big Difference**: `WHERE` filters individual rows **before** grouping. `HAVING` filters groups **after** the aggregation is calculated.

### Putting it all Together: `WHERE`, `GROUP BY`, and `HAVING`
When you combine all these clauses, you can perform highly specific data analysis tasks.

**Pandas:**
```python
# 1. Filter rows (WHERE)
# 2. Group by city and calculate mean (GROUP BY)
# 3. Filter groups (HAVING)
result = df[df['age'] >= 18].groupby('city')['age'].mean()
result = result[result > 30]
```

**SQL:**
```sql
SELECT city, AVG(age)
FROM users
WHERE age >= 18
GROUP BY city
HAVING AVG(age) > 30;
```

> **Final Written Order**:
> 1. `SELECT`
> 2. `FROM`
> 3. `WHERE`
> 4. `GROUP BY`
> 5. `HAVING`
> 6. `ORDER BY`
> 
> **Final Execution Order**:
> 1. `FROM`: Identify the table.
> 2. `WHERE`: Filter individual rows.
> 3. `GROUP BY`: Group the remaining rows.
> 4. `HAVING`: Filter the calculated groups.
> 5. `SELECT`: Determine which columns/aggregates to show.
> 6. `ORDER BY`: Sort the final result (if present).

### Advanced Grouping: Rollups and Cubes
PostgreSQL (and other modern RDBMS) support extensions to `GROUP BY` for generating "subtotal" rows.
- **`ROLLUP`**: Creates hierarchical subtotals (e.g., total by country, then total by city).
- **`CUBE`**: Creates all possible combinations of subtotals.
- **`GROUPING SETS`**: Lets you list the exact grouping levels you want, without generating every combination.

```sql
-- Generates subtotals for each city AND a grand total
SELECT COALESCE(city, 'Total'), COUNT(*)
FROM users
GROUP BY ROLLUP(city);

-- Generates subtotals for all possible combinations of cities 
-- and countries
SELECT 
    COALESCE(country, 'ALL'), 
    COALESCE(city, 'ALL'), 
    COUNT(*)
FROM users
GROUP BY CUBE(country, city);

-- Generates only the groupings you list:
-- (country, city), (country), and the grand total ()
SELECT 
    COALESCE(country, 'ALL') AS country, 
    COALESCE(city, 'ALL') AS city, 
    COUNT(*)
FROM users
GROUP BY GROUPING SETS (
    (country, city), -- by country + city
    (country),       -- by country
    ()               -- grand total
);
```

---

## 7. Advanced Querying

As your analysis becomes more complex, you will need tools to handle nested logic, temporary results, and analytical calculations across rows.

### Subqueries
A subquery is a query nested inside another query. They are often used in the `WHERE`, `FROM`, or `SELECT` clauses.

- **Scalar Subquery**: Returns a single value (1 row, 1 column).
- **Multi-row Subquery**: Returns a list of values (multiple rows, 1 column).
- **Correlated Subquery**: A subquery that refers to a column from the outer query.

**SQL Example (Multi-row):**
Find users who have placed an order.
```sql
SELECT name 
FROM users 
WHERE user_id IN (SELECT DISTINCT user_id FROM orders);
```

**Pandas Equivalent:**
```python
users[users['user_id'].isin(orders['user_id'].unique())]['name']
```

---

### Common Table Expressions (CTEs) with `WITH`
CTEs allow you to define a temporary result set that you can reference within a `SELECT`, `INSERT`, `UPDATE`, or `DELETE` statement. They are much more readable than nested subqueries and are equivalent to assigning a temporary DataFrame in Pandas.

**SQL Example:**
```sql
WITH regional_sales AS (
    SELECT region, SUM(amount) AS total_sales
    FROM orders
    GROUP BY region
)
SELECT region
FROM regional_sales
WHERE total_sales > 10000;
```

**Pandas Equivalent:**
```python
regional_sales = orders.groupby('region')['amount'].sum().reset_index()
result = regional_sales[regional_sales['amount'] > 10000]['region']
```

---

### Window Functions
Window functions perform a calculation across a set of table rows that are somehow related to the current row. Unlike aggregate functions, they do not cause rows to become grouped into a single output row.

You can pair standard aggregates with window clauses to show per-row context (no collapsing of rows). Common patterns:

- Partitioned totals: `SUM(sales) OVER (PARTITION BY region)` gives each row the total sales for its region.
- Running totals: `SUM(sales) OVER (PARTITION BY region ORDER BY date)` accumulates within each region as dates progress.
- Moving averages: `AVG(sales) OVER (ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW)` gives a 7-day rolling average.

**SQL Examples (Aggregates as Windows):**
```sql
SELECT
    region,
    date,
    sales,
    SUM(sales) OVER (PARTITION BY region)            AS region_total, -- windows is from the first row to the last row
    SUM(sales) OVER (PARTITION BY region ORDER BY date) AS running_total, -- windows is from the first row to the current row
    AVG(sales) OVER (
        ORDER BY date
        ROWS BETWEEN 6 PRECEDING AND CURRENT ROW
    ) AS seven_day_avg -- windows is from the 6 days before the current day to the current day
FROM daily_sales;
```

**Pandas Equivalent:**
```python
# Region totals (broadcast, no collapse)
df['region_total'] = df.groupby('region')['sales'].transform('sum')

# Running total within region
df['running_total'] = (
    df.sort_values(['region', 'date'])
      .groupby('region')['sales']
      .cumsum()
)

# 7-day rolling average by date (no partition in this simple example)
df = df.sort_values('date')
df['seven_day_avg'] = df['sales'].rolling(window=7, min_periods=1).mean()
```

You can DRY up repeated window clauses using the `WINDOW` keyword:
```sql
SELECT
    region,
    date,
    sales,
    SUM(sales) OVER sales_win AS running_total,
    AVG(sales) OVER sales_win AS running_avg
FROM daily_sales
WINDOW sales_win AS (PARTITION BY region ORDER BY date);
```

These functions are most useful when you need to perform a calculation on a row-by-row basis, without collapsing the rows into a single output row.

| Function | Description |
| :--- | :--- |
| `ROW_NUMBER()` | Assigns a unique sequential integer to rows. |
| `RANK()` | Assigns a rank with gaps for ties. |
| `DENSE_RANK()` | Assigns a rank without gaps for ties. |
| `LEAD()` / `LAG()` | Accesses data from a subsequent or previous row. |
| `NTILE(n)` | Divides rows into `n` buckets. |

**SQL Example (Ranking - RANK vs DENSE_RANK vs ROW_NUMBER):**
```sql
SELECT name, department, salary,
       RANK() OVER(PARTITION BY department ORDER BY salary DESC) as rank,
       DENSE_RANK() OVER(PARTITION BY department ORDER BY salary DESC) as dense_rank,
       ROW_NUMBER() OVER(PARTITION BY department ORDER BY salary DESC) as row_num
FROM employees;
```

**Pandas Equivalent:**
```python
# RANK (Gaps for ties: 1, 2, 2, 4)
df['rank'] = df.groupby('department')['salary'].rank(method='min', ascending=False)

# DENSE_RANK (No gaps for ties: 1, 2, 2, 3)
df['dense_rank'] = df.groupby('department')['salary'].rank(method='dense', ascending=False)

# ROW_NUMBER (Unique index even for ties: 1, 2, 3, 4)
df['row_num'] = df.groupby('department')['salary'].rank(method='first', ascending=False)
```

**SQL Example (NTILE):**
Divides the result set into a specified number of approximately equal groups.
```sql
SELECT name, salary,
       NTILE(4) OVER(ORDER BY salary DESC) as quartile
FROM employees;
```

**Pandas Equivalent:**
```python
# Using qcut to create 4 equal-sized buckets
df['quartile'] = pd.qcut(df['salary'].rank(method='first'), q=4, labels=[1, 2, 3, 4])
```

**SQL Example (Lead/Lag):**
```sql
SELECT date, sales,
       LAG(sales) OVER(ORDER BY date) as previous_day_sales,
       LEAD(sales) OVER(ORDER BY date) as next_day_sales
FROM daily_sales;
```

**Pandas Equivalent:**
```python
df['previous_day_sales'] = df['sales'].shift(1)
df['next_day_sales'] = df['sales'].shift(-1)
```

---

## 8. Advanced Topics for Data Analysts

This final section covers more structural SQL concepts and how SQL integrates with Python for real-world analysis.

### Data Cleaning and Transformation (Pivot/Unpivot)
Reshaping data is a common task. In Pandas, you use `df.pivot()` and `df.melt()`. In standard SQL, you often use `CASE` statements or database-specific functions (like PostgreSQL's `crosstab` from the `tablefunc` extension).

**Pivot Pattern (using `CASE`):**
Converting rows to columns.
```sql
SELECT 
    year,
    SUM(CASE WHEN month = 1 THEN sales ELSE 0 END) AS jan_sales,
    SUM(CASE WHEN month = 2 THEN sales ELSE 0 END) AS feb_sales
FROM monthly_sales
GROUP BY year;
```

**Unpivot Pattern (using `UNION ALL`):**
Converting columns back to rows.
```sql
SELECT year, 'Jan' AS month, jan_sales AS sales FROM yearly_sales
UNION ALL
SELECT year, 'Feb' AS month, feb_sales AS sales FROM yearly_sales;
```

---

### DDL (Data Definition Language) for Analysts
While Data Engineers usually handle the database structure, analysts often need to create temporary or permanent tables and views to store their results.

- **`CREATE TABLE AS SELECT` (CTAS)**: Creates a new table based on the result of a query.
  ```sql
  CREATE TABLE high_value_customers AS
  SELECT * FROM customers WHERE lifetime_value > 1000;
  ```
- **`CREATE VIEW`**: Creates a "virtual table"—a saved query that you can treat like a table. It doesn't store data, just the query.
  ```sql
  CREATE VIEW active_orders AS
  SELECT * FROM orders WHERE status = 'shipped';
  ```

---

### Database Objects and Concepts
- **Indexes**: Used to speed up data retrieval. Think of it like a book's index—instead of scanning every page (table scan), the database looks up the location in the index first.
- **Triggers**: Automated actions that happen when data is inserted, updated, or deleted.
- **Stored Procedures**: A group of SQL statements that can be saved and reused.
- **Transactions**: Ensuring multiple steps happen together. If one fails, they all fail (`BEGIN`, `COMMIT`, `ROLLBACK`).

---

### SQL and Python Integration
In professional environments, you'll rarely use just one tool. You'll use Python to connect to a SQL database, pull data, and then use Pandas for further analysis.

#### Key Libraries:
- **`psycopg2`**: The most popular PostgreSQL adapter for Python.
- **`SQLAlchemy`**: A powerful SQL Toolkit and ORM (Object-Relational Mapper) that allows you to write Python code that generates SQL.

**Example: Connecting to PostgreSQL with SQLAlchemy**
```python
import pandas as pd
from sqlalchemy import create_engine

# 1. Create a connection string
# Format: postgresql://username:password@host:port/database
engine = create_engine('postgresql://user:pass@localhost:5432/my_db')

# 2. Run a SQL query and load it directly into a Pandas DataFrame
df = pd.read_sql_query("SELECT * FROM users WHERE age > 25", engine)

# 3. Perform analysis in Pandas
print(df.head())
```




