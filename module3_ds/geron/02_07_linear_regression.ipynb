{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dde8324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69ea5b6",
   "metadata": {},
   "source": [
    "Given a system of linear equations:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "a_{11}x_1 + a_{12}x_2 + \\cdots + a_{1n}x_n &= b_1 \\\\\n",
    "a_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n &= b_2 \\\\\n",
    "\\vdots \\quad \\quad \\quad \\quad \\quad &= \\vdots \\\\\n",
    "a_{m1}x_1 + a_{m2}x_2 + \\cdots + a_{mn}x_n &= b_m\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This is equivalent to writing **$A\\mathbf{x} = \\mathbf{b}$**, where:\n",
    "$$\n",
    "A = \\begin{pmatrix}\n",
    "a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n",
    "a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{m1} & a_{m2} & \\cdots & a_{mn}\n",
    "\\end{pmatrix} \\in \\mathbb{R}^{m \\times n}\n",
    "$$\n",
    "$$\n",
    "x = \\begin{pmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{pmatrix} \\in \\mathbb{R}^n\n",
    "$$\n",
    "$$\n",
    "b = \\begin{pmatrix}\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "\\vdots \\\\\n",
    "b_m\n",
    "\\end{pmatrix} \\in \\mathbb{R}^m\n",
    "$$\n",
    "\n",
    "If $A$ is **square** ($m = n$) and **invertible**, we can solve:\n",
    "\n",
    "$$\\mathbf{x} = A^{-1}\\mathbf{b}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a179426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.3,  -0.4, -34. ])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# System: \n",
    "# 170x1 + 25x2 + x3 = 7\n",
    "# 180x1 + 30x2 + x3 = 8\n",
    "# 200x1 + 35x2 + x3 = 12\n",
    "\n",
    "A = np.array([\n",
    "    [170, 25, 1],\n",
    "    [180, 30, 1],\n",
    "    [200, 35, 1]\n",
    "])\n",
    "b = np.array([7, 8, 12])\n",
    "\n",
    "x = np.linalg.inv(A) @ b\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ae11c031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.3,  -0.4, -34. ])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solve() (more numerically stable)\n",
    "x = np.linalg.solve(A, b)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1891cdc2",
   "metadata": {},
   "source": [
    "- $A$ is not square → $A^{-1}$ **doesn't exist**\n",
    "- We have **more observations than features** ($m > n$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2465216",
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Last 2 dimensions of the array must be square",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLinAlgError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      7\u001b[39m A = np.array([\n\u001b[32m      8\u001b[39m     [\u001b[32m170\u001b[39m, \u001b[32m25\u001b[39m, \u001b[32m1\u001b[39m],\n\u001b[32m      9\u001b[39m     [\u001b[32m180\u001b[39m, \u001b[32m30\u001b[39m, \u001b[32m1\u001b[39m],\n\u001b[32m     10\u001b[39m     [\u001b[32m200\u001b[39m, \u001b[32m35\u001b[39m, \u001b[32m1\u001b[39m],\n\u001b[32m     11\u001b[39m     [\u001b[32m160\u001b[39m, \u001b[32m20\u001b[39m, \u001b[32m1\u001b[39m],\n\u001b[32m     12\u001b[39m ])\n\u001b[32m     13\u001b[39m b = np.array([\u001b[32m7\u001b[39m, \u001b[32m8\u001b[39m, \u001b[32m12\u001b[39m, \u001b[32m15\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m x = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m @ b\n\u001b[32m     16\u001b[39m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\ds\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:642\u001b[39m, in \u001b[36minv\u001b[39m\u001b[34m(a)\u001b[39m\n\u001b[32m    538\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    539\u001b[39m \u001b[33;03mCompute the inverse of a matrix.\u001b[39;00m\n\u001b[32m    540\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    639\u001b[39m \n\u001b[32m    640\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    641\u001b[39m a, wrap = _makearray(a)\n\u001b[32m--> \u001b[39m\u001b[32m642\u001b[39m \u001b[43m_assert_stacked_square\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    643\u001b[39m t, result_t = _commonType(a)\n\u001b[32m    645\u001b[39m signature = \u001b[33m'\u001b[39m\u001b[33mD->D\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33md->d\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\ds\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:246\u001b[39m, in \u001b[36m_assert_stacked_square\u001b[39m\u001b[34m(*arrays)\u001b[39m\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[33m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m-dimensional array given. Array must be \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    244\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mat least two-dimensional\u001b[39m\u001b[33m'\u001b[39m % a.ndim)\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m m != n:\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[33m'\u001b[39m\u001b[33mLast 2 dimensions of the array must be square\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mLinAlgError\u001b[39m: Last 2 dimensions of the array must be square"
     ]
    }
   ],
   "source": [
    "# System: \n",
    "# 170x1 + 25x2 + x3 = 7\n",
    "# 180x1 + 30x2 + x3 = 8\n",
    "# 200x1 + 35x2 + x3 = 12\n",
    "# 160x1 + 20x2 + x3 = 15\n",
    "\n",
    "A = np.array([\n",
    "    [170, 25, 1],\n",
    "    [180, 30, 1],\n",
    "    [200, 35, 1],\n",
    "    [160, 20, 1],\n",
    "])\n",
    "b = np.array([7, 8, 12, 15])\n",
    "\n",
    "x = np.linalg.inv(A) @ b\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58e44bc",
   "metadata": {},
   "source": [
    "to find $\\mathbf{x}$ we need to minimize $f(\\mathbf{x}) = \\| A\\mathbf{x} - \\mathbf{b} \\|$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(x) &= \\|Ax - b\\|^2 \\\\\n",
    "&= (Ax - b)^T (Ax - b) \\\\\n",
    "&= (x^T A^T - b^T)(Ax - b) \\\\\n",
    "&= x^T A^T Ax - x^T A^T b - b^T Ax + b^T b \\\\\n",
    "&= x^T A^T Ax - 2b^T Ax + b^T b \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Derivative rules can be applied\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial x} a^{T}x &= a \\\\\n",
    "\\frac{\\partial}{\\partial x} x^{T}Ax &= (A + A^T)x\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Taking the derivative and setting to zero:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial f}{\\partial x} &= \\frac{\\partial}{\\partial x} (x^T A^T Ax - 2b^T Ax + b^T b) \\\\\n",
    "&= 2A^T Ax - 2A^T b \\\\\n",
    "&= 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "So we must have \n",
    "$$\\begin{aligned}\n",
    "A^TA\\mathbf{x} &= A^T\\mathbf{b} \\\\\n",
    "\\mathbf{x} &= (A^TA)^{-1}A^T\\mathbf{b} \\quad \\text{(if } A^TA \\text{ is invertible)}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309d880f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.9,  -2.5, -80.5])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# System: \n",
    "# 170x1 + 25x2 + x3 = 7\n",
    "# 180x1 + 30x2 + x3 = 8\n",
    "# 200x1 + 35x2 + x3 = 12\n",
    "# 160x1 + 20x2 + x3 = 15\n",
    "\n",
    "A = np.array([\n",
    "    [170, 25, 1],\n",
    "    [180, 30, 1],\n",
    "    [200, 35, 1],\n",
    "    [160, 20, 1],\n",
    "])\n",
    "b = np.array([7, 8, 12, 15])\n",
    "\n",
    "x = np.linalg.pinv(A) @ b\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4d35bc",
   "metadata": {},
   "source": [
    "**Problems with $(A^TA)^{-1}A^T\\mathbf{b}$:**\n",
    "\n",
    "1. High Computational Cost for inverting $A^TA$\n",
    "2. $A^TA$ can be **ill-conditioned** (nearly singular). Small changes in data → huge changes in solution\n",
    "3. Need to store $n \\times n$ matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "affc12c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(-80.50000000000003), array([ 0.9, -2.5]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(A[:, :-1], b)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac61c55",
   "metadata": {},
   "source": [
    "# Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c5155b",
   "metadata": {},
   "source": [
    "\n",
    "Instead of computing the inverse, **iteratively update** parameters to minimize the cost function.\n",
    "\n",
    "**Cost Function (MSE)**:\n",
    "$$J(\\mathbf{x}) = \\frac{1}{2m} \\|\\mathbf{b} - A\\mathbf{x}\\|^2$$\n",
    "\n",
    "Where $m$ is the size of sample\n",
    "\n",
    "### The Gradient\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\nabla J(\\mathbf{x}) &= \\frac{\\partial J}{\\partial \\mathbf{x}}  \\\\\n",
    " &= \\frac{\\partial}{\\partial \\mathbf{x}} \\left( \\frac{1}{2m} \\|\\mathbf{b} - A\\mathbf{x}\\|^2 \\right) \\\\ \n",
    " &= \\frac{1}{2m} \\frac{\\partial}{\\partial \\mathbf{x}} \\|\\mathbf{b} - A\\mathbf{x}\\|^2 \\\\ \n",
    " &= -\\frac{1}{m} A^T(\\mathbf{b} - A\\mathbf{x}) \\\\\n",
    "&= \\frac{1}{m} A^T(A\\mathbf{x} - \\mathbf{b})\n",
    "\\end{aligned}$$\n",
    "\n",
    "### Update Rule\n",
    "\n",
    "$$\\mathbf{x} := \\mathbf{x} - \\eta \\ \\nabla J(\\mathbf{x})$$\n",
    "\n",
    "Where $\\eta$ is the **learning rate**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f1dc87",
   "metadata": {},
   "source": [
    "### Why Gradient Descent?\n",
    "\n",
    "| Aspect | Normal Equation | Gradient Descent |\n",
    "|--------|----------------|------------------|\n",
    "| Complexity | $O(n^3)$ | $O(k \\cdot m \\cdot n)$ |\n",
    "| Features | Works for small $n$ | Scales to large $n$ |\n",
    "| Inversion | Required | Not needed |\n",
    "| Learning Rate | N/A | Need to tune $\\alpha$ |\n",
    "| Iterations | 1 (closed-form) | Many |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ac84933a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200, 2), (200, 2))"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng(seed=42)\n",
    "m = 200  # number of instances\n",
    "X = 2 * rng.random((m, 2))  # column vector\n",
    "y = 4 + 3 * X + rng.standard_normal((m, 1))  # column vector\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c2d3c63a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rng = np.random.default_rng(seed=42)\n",
    "m = 2000  # number of instances\n",
    "\n",
    "boylar = rng.normal(175, 10, size=(m,1))\n",
    "yashlar = rng.uniform(18, 60, size=(m,1))\n",
    "# maashlar = rng.exponential(scale=4, size=(m,1))\n",
    "# maashlar = rng.chisquare(df=6, size=(m,1))\n",
    "maashlar = rng.gamma(shape=3.5, scale=2, size=(m,1))\n",
    "\n",
    "plt.hist(maashlar, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "f6786c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.        , 1.78047171, 0.19778537],\n",
       "        [1.        , 1.64600159, 0.36687386],\n",
       "        [1.        , 1.82504512, 0.40973487],\n",
       "        ...,\n",
       "        [1.        , 1.740846  , 0.23681841],\n",
       "        [1.        , 1.69691782, 0.56835309],\n",
       "        [1.        , 1.97141526, 0.38183515]], shape=(2000, 3)),\n",
       " array([[0.61006609],\n",
       "        [0.64261833],\n",
       "        [0.39956131],\n",
       "        ...,\n",
       "        [0.39419609],\n",
       "        [1.47388686],\n",
       "        [0.21100565]], shape=(2000, 1)))"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import add_dummy_feature\n",
    "\n",
    "X = np.hstack([boylar/100, yashlar/100])\n",
    "X_b = add_dummy_feature(X)\n",
    "# X_b = np.c_[X, np.ones((m, 1))]\n",
    "\n",
    "y = maashlar/10\n",
    "\n",
    "X_b, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d770db87",
   "metadata": {},
   "source": [
    "## Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "850ee108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.70714298],\n",
       "       [ 0.01070541],\n",
       "       [-0.06718738]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta = 0.1  # learning rate\n",
    "n_epochs = 30000\n",
    "m = len(X_b)  # number of instances\n",
    "\n",
    "rng = np.random.default_rng(seed=42)\n",
    "theta = rng.standard_normal((3, 1))  # randomly initialized model parameters\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    gradients = 1 / m * X_b.T @ (X_b @ theta - y)\n",
    "    theta = theta - eta * gradients\n",
    "    \n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "0cfb4edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.70236187]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = np.array([\n",
    "    [1.        , 1.75, 0.35]\n",
    "])\n",
    "\n",
    "X_test @ theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4122c0",
   "metadata": {},
   "source": [
    "**Pros**: Stable convergence  \n",
    "**Cons**: Slow for large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb4814f",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "9e20a440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.77150148],\n",
       "       [-0.07764057],\n",
       "       [ 0.16762074]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "t0, t1 = 5, 50  # learning schedule hyperparameters\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "rng = np.random.default_rng(seed=42)\n",
    "theta = rng.standard_normal((3, 1))  # randomly initialized model parameters\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for iteration in range(m):\n",
    "\n",
    "        random_index = rng.integers(m)\n",
    "        xi = X_b[random_index : random_index + 1]\n",
    "        yi = y[random_index : random_index + 1]\n",
    "        gradients = 2 * xi.T @ (xi @ theta - yi)  # for SGD, do not divide by m\n",
    "        eta = learning_schedule(epoch * m + iteration)  # learning rate\n",
    "        theta = theta - eta * gradients\n",
    "\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "0c4d1ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.7144053]), array([ 0.02406006, -0.06317182]))"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd_reg = SGDRegressor(\n",
    "    max_iter=10000, \n",
    "    # tol=1e-5, \n",
    "    penalty=None, \n",
    "    eta0=0.1,\n",
    "    n_iter_no_change=1000, \n",
    "    random_state=42\n",
    ")\n",
    "sgd_reg.fit(X, y.ravel())  # y.ravel() because fit() expects 1D targets\n",
    "sgd_reg.intercept_, sgd_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03515f59",
   "metadata": {},
   "source": [
    "**Pros**: Fast, can escape local minima  \n",
    "**Cons**: Noisy, never truly converges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947fe9be",
   "metadata": {},
   "source": [
    "## Mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "5fcd50c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.70797365],\n",
       "       [ 0.01056922],\n",
       "       [-0.06719565]])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extra code – this cell generates Figure 4–11\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "n_epochs = 1000\n",
    "minibatch_size = 20\n",
    "n_batches_per_epoch = ceil(m / minibatch_size)\n",
    "\n",
    "rng = np.random.default_rng(seed=42)\n",
    "theta = rng.standard_normal((3, 1))  # randomly initialized model parameters\n",
    "\n",
    "t0, t1 = 200, 1000  # learning schedule hyperparameters\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "theta_path_mgd = []\n",
    "for epoch in range(n_epochs):\n",
    "    shuffled_indices = rng.permutation(m)\n",
    "    X_b_shuffled = X_b[shuffled_indices]\n",
    "    y_shuffled = y[shuffled_indices]\n",
    "    for iteration in range(0, n_batches_per_epoch):\n",
    "        idx = iteration * minibatch_size\n",
    "        xi = X_b_shuffled[idx : idx + minibatch_size]\n",
    "        yi = y_shuffled[idx : idx + minibatch_size]\n",
    "        gradients = 2 / minibatch_size * xi.T @ (xi @ theta - yi)\n",
    "        eta = learning_schedule(epoch * n_batches_per_epoch + iteration)\n",
    "        theta = theta - eta * gradients\n",
    "\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3282cdae",
   "metadata": {},
   "source": [
    "**Pros**: Best of both worlds (stable + efficient)  \n",
    "**Cons**: Extra hyperparameter (batch size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
