# 2.2. Data Manipulation with Pandas

## Introduction to Pandas

Pandas is a powerful and popular open-source Python library for data manipulation and analysis. It provides high-performance, easy-to-use data structures, and data analysis tools. The name "Pandas" is derived from "Panel Data," an econometrics term for multidimensional, structured data sets.

Pandas is a cornerstone of data science in Python, offering two primary data structures that are fundamental to data manipulation: the `Series` and the `DataFrame`.

## Pandas Data Structures: Series

A **Series** is a one-dimensional labeled array capable of holding any data type (integers, strings, floating-point numbers, Python objects, etc.). It is similar to a column in a spreadsheet or a single column from a database table.

**Example: Creating a Series**

To create a Series, you can pass a list of values to the `pd.Series()` constructor.

```python
import pandas as pd

# Creating a Series from a list
data = [2,-1,3,5]
s = pd.Series(data)
print(s)
```

**Output:**

```
0    2
1   -1
2    3
3    5
dtype: int64
```

`Series` objects behave much like one-dimensional NumPy `ndarray`s, and you can often pass them as parameters to NumPy functions:
```python
import numpy as np
np.exp(s)
```

**Output:**

```
0      7.389056
1      0.367879
2     20.085537
3    148.413159
dtype: float64
```

Arithmetic operations on `Series` are also possible, and they apply *elementwise*, just like for `ndarray`s:
```python
s + [1000,2000,3000,4000]
```

**Output:**

```
0    1002
1    1999
2    3003
3    4005
dtype: int64
```

Similar to NumPy, if you add a single number to a `Series`, that number is added to all items in the `Series`. This is called *broadcasting*:

```python
s + 1000
```

**Output:**

```
0    1002
1     999
2    1003
3    1005
dtype: int64
```

The same is true for all binary operations such as `*` or `/`, and even conditional operations:

```python
s < 0
```

**Output:**

```
0    False
1     True
2    False
3    False
dtype: bool
```

### Index
By default, index is simply the rank of the item in the `Series` (starting from `0`) but you can also set the index labels manually:

```python
s2 = pd.Series(
    [68, 83, 112, 68], 
    index=["alice", "bob", "charles", "darwin"]
)
s2
```

**Output:**

```
alice       68
bob         83
charles    112
darwin      68
dtype: int64
```

> **Note:** You can also access the index of the `Series` using the `index` attribute.
> s2.index
> 
> **Output:**
> ```
> Index(['alice', 'bob', 'charles', 'darwin'], dtype='object')
> ```

> **Note:** If you initialize a `Series` with a scalar value and a list of indices, the scalar value is repeated for each index.
>
> ```python
> pd.Series(42, ["life", "universe", "everything"])
> ```
>
> **Output:**
>
> ```
> life          42
> universe      42
> everything    42
> dtype: int64
> ```

You can then use the `Series` just like a `dict`:

```python
s2["bob"]
```

**Output:**

```
83
```

You can also use the `Series` as a dictionary to set values:

```python
s2["bob"] = 100
print(s2)
```

To make it clear when you are accessing by label or by integer location, it is recommended to always use the `loc` attribute when accessing by label, and the `iloc` attribute when accessing by integer location:

```python
s2.loc["bob"]
```

**Output:**

```
83
```

```python
s2.iloc[1]
```

**Output:**

```
83
```
Slicing a Series is similar to slicing a list:

```python
s2.iloc[1:3]
```

**Output:**

```
bob         83
charles    112
dtype: int64
```

### Alternative Constructor

You can also create a `Series` from a dictionary:

```python
weights = {"alice": 68, "bob": 83, "colin": 86, "darwin": 68}
s3 = pd.Series(weights)
s3
```

**Output:**

```
alice     68
bob       83
colin     86
darwin    68
dtype: int64
```

### Automatic Alignment

When you perform arithmetic operations on `Series`, the operations are performed element-wise, and the indices of the `Series` are automatically aligned.

```python
s3 + s2
```

**Output:**

```
alice      136.0
bob        166.0
charles      NaN
colin        NaN
darwin     136.0
dtype: float64
```

> **Note:** NaN stands for "Not a Number" and is used to represent missing values.

### Name of the Series

You can also set the name of the `Series` using the `name` parameter:

```python
s6 = pd.Series([83, 68], index=["bob", "alice"], name="weights")
s6
```

**Output:**

```
bob      83
alice    68
Name: weights, dtype: int64
```

## Pandas Data Structures: DataFrame

A **DataFrame** is a two-dimensional labeled data structure with columns of potentially different types. It is similar to a spreadsheet, a SQL table, or a dictionary of Series objects. It is the most commonly used pandas object.

## Creating a DataFrame

You can create a DataFrame from a dictionary of lists, where each list represents a column.

```python
import pandas as pd

# Creating a DataFrame from a dictionary
data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'David'],
    'Age': [25, 30, 35, 40],
    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston']
}
df = pd.DataFrame(data)
print(df)
```

**Output:**

```
      Name  Age         City
0    Alice   25     New York
1      Bob   30  Los Angeles
2  Charlie   35      Chicago
3    David   40      Houston
```

You can also create a DataFrame from a dictionary of `Series` objects:

```python
weight_series = pd.Series([68, 83, 112], index=["alice", "bob", "charles"])
birthyear_series = pd.Series([1984, 1985, 1992], index=["bob", "alice", "charles"], name="year")
children_series = pd.Series([0, 3], index=["charles", "bob"])
hobby_series = pd.Series(["Biking", "Dancing"], index=["alice", "bob"])

people_dict = {
    "weight": weight_series,
    "birthyear": birthyear_series,
    "children": children_series,
    "hobby": hobby_series,
}
people = pd.DataFrame(people_dict)
people
```

**Output:**

```
         weight  birthyear  children    hobby
alice        68       1985       NaN   Biking
bob          83       1984       3.0  Dancing
charles     112       1992       0.0      NaN
```

> **Note:** The `Series` were automatically aligned based on the indices and the missing values are represented as `NaN`. Also note that the name `year` was dropped from the `birthyear` `Series`.

> **Note:** The index of the `DataFrame` can be accessed using the `index` attribute.
> ```python
> people.index
> ```
> **Output:**
> ```
> Index(['alice', 'bob', 'charles'], dtype='object')
> ```
> ```python
> people.columns
> ```
> **Output:**
> ```
> Index(['weight', 'birthyear', 'children', 'hobby'], dtype='object')
> ```

You can access columns using the `[]` operator. They are returned as `Series` objects:

```python
people["birthyear"]
```

**Output:**

```
alice      1985
bob        1984
charles    1992
Name: birthyear, dtype: int64
```

You can also get multiple columns at once:
```python
people[["birthyear", "hobby"]]
```

**Output:**

```
         birthyear    hobby
alice       1985   Biking
bob         1984  Dancing
charles     1992      NaN
dtype: object
```

If you pass a list of columns and/or index row labels to the `DataFrame` constructor.

```python
d2 = pd.DataFrame(
        people_dict,
        columns=["birthyear", "weight", "height"], # reorder the columns
        index=["bob", "alice", "eugene"] # reorder the index
     )
d2
```
**Output:**

```
        birthyear  weight height
bob        1984.0    83.0    NaN
alice      1985.0    68.0    NaN
eugene        NaN     NaN    NaN
```

### Alternative Constructors

Another convenient way to create a `DataFrame` is to pass all the values to the constructor as an `ndarray`, or a list of lists, and specify the column names and row index labels separately:

```python
values = [
            [1985, np.nan, "Biking",   68],
            [1984, 3,      "Dancing",  83],
            [1992, 0,      np.nan,    112]
         ]
d3 = pd.DataFrame(
        values,
        columns=["birthyear", "children", "hobby", "weight"],
        index=["alice", "bob", "charles"]
     )
d3
```

**Output:**

```
         birthyear  children    hobby  weight
alice         1985       NaN   Biking      68
bob           1984       3.0  Dancing      83
charles       1992       0.0      NaN     112
```

It is also possible to create a `DataFrame` with a dictionary (or list) of dictionaries (or lists):

```python
people = pd.DataFrame({
    "birthyear": {"alice": 1985, "bob": 1984, "charles": 1992},
    "hobby": {"alice": "Biking", "bob": "Dancing"},
    "weight": {"alice": 68, "bob": 83, "charles": 112},
    "children": {"bob": 3, "charles": 0}
})
people
```

**Output:**

```
         birthyear    hobby  weight  children
alice         1985   Biking      68       NaN
bob           1984  Dancing      83       3.0
charles       1992      NaN     112       0.0
```

## Reading and Writing Data

Pandas provides a rich set of functions for reading data from various file formats and writing data back to them. The most common formats include CSV, Excel, JSON, and SQL databases.

**Example: Reading a CSV file**

Let's assume you have a CSV file named `data.csv` with the following content:

```csv
Name,Age,City
Alice,25,New York
Bob,30,Los Angeles
Charlie,35,Chicago
David,40,Houston
```

You can read this file into a DataFrame using `pd.read_csv()`:

```python
import pandas as pd

df = pd.read_csv('data.csv')
print(df)
```

**Example: Writing to a CSV file**

You can write a DataFrame to a CSV file using the `to_csv()` method.

```python
df.to_csv('output.csv', index=False)
```

The `index=False` argument prevents pandas from writing the DataFrame index as a column in the CSV file.

## Multi-Index DataFrame
```python
d5 = pd.DataFrame(
  {
    ("public", "birthyear"):
        {("Paris","alice"): 1985, ("Paris","bob"): 1984, ("London","charles"): 1992},
    ("public", "hobby"):
        {("Paris","alice"): "Biking", ("Paris","bob"): "Dancing"},
    ("private", "weight"):
        {("Paris","alice"): 68, ("Paris","bob"): 83, ("London","charles"): 112},
    ("private", "children"):
        {("Paris", "alice"): np.nan, ("Paris","bob"): 3, ("London","charles"): 0}
  }
)
d5
```

**Output:**

```
                public              private         
                birthyear    hobby  weight children
Paris  alice        1985   Biking      68      NaN
       bob          1984  Dancing      83      3.0
London charles      1992      NaN     112      0.0
```

You can now get a `DataFrame` containing all the `"public"` columns very simply:
```python
d5["public"]
```

**Output:**

```
                birthyear    hobby
Paris  alice         1985   Biking
       bob           1984  Dancing
London charles       1992      NaN
```

```python
d5["public", "hobby"]  # Same result as d5["public"]["hobby"]
```

**Output:**

```
Paris   alice       Biking
        bob        Dancing
London  charles        NaN
Name: (public, hobby), dtype: object
```

> **Note:** Multi-Index DataFrame can also be created using the `MultiIndex` class.
> ```python
> columns = pd.MultiIndex.from_tuples([('public', 'birthyear'), ('public', 'hobby'), ('private', 'weight'), ('private', 'children')])
> index = pd.MultiIndex.from_tuples([('Paris', 'alice'), ('Paris', 'bob'), ('London', 'charles')])
> data = [
>     [1985, 'Biking', 68, 'NaN'],
>     [1984, 'Dancing', 83, 3.0],
>     [1992, 'NaN', 112, 0.0]
>     ]
> d5 = pd.DataFrame(data, index=index, columns=columns)
> d5
> ```
> **Output:**
> ```
>                   public              private         
>                   birthyear    hobby  weight children
> Paris  alice        1985   Biking      68      NaN
>        bob          1984  Dancing      83      3.0
> London charles      1992      NaN     112      0.0
> ```
> Or even simpler:
> ```python
> d5 = pd.DataFrame(data, index=[['Paris', 'Paris', 'London'], ['alice', 'bob', 'charles']], columns=[['public', 'public', 'private', 'private'], ['birthyear', 'hobby', 'weight', 'children']])
> d5
> ```
> **Output:**
> ```
>                   public              private         
>                   birthyear    hobby  weight children
> Paris  alice        1985   Biking      68      NaN
>        bob          1984  Dancing      83      3.0
> London charles      1992      NaN     112      0.0
> ```

There are two levels of columns, and two levels of indices. We can drop a column level by calling `droplevel()` (the same goes for indices):

```python
d5.columns = d5.columns.droplevel(level = 0)
d5
```

**Output:**

```
                birthyear    hobby  weight  children
Paris  alice         1985   Biking      68       NaN
       bob           1984  Dancing      83       3.0
London charles       1992      NaN     112       0.0
```

```python
d6 = d5.T
d6
```

**Output:**

```
            Paris           London
            alice      bob charles
birthyear    1985     1984    1992
hobby      Biking  Dancing     NaN
weight         68       83     112
children      NaN      3.0     0.0
```

### Stacking and Unstacking

Calling the `stack()` method will push the lowest column level after the lowest index:

```python
d7 = d6.stack()
d7
```

**Output:**

```python
                     Paris London
birthyear alice       1985    NaN
          bob         1984    NaN
          charles      NaN   1992
hobby     alice     Biking    NaN
          bob      Dancing    NaN
weight    alice         68    NaN
          bob           83    NaN
          charles      NaN    112
children  bob          3.0    NaN
          charles      NaN    0.0
```

Calling `unstack()` will do the reverse, once again creating many `NaN` values.

> **Note:** Many `NaN` values appeared. This is because many new combinations did not exist before.

```python
d8 = d7.unstack()
d8
```

**Output:**
```
            Paris                  London             
            alice      bob charles  alice  bob charles
birthyear    1985     1984     NaN    NaN  NaN    1992
children      NaN      3.0     NaN    NaN  NaN     0.0
hobby      Biking  Dancing     NaN    NaN  NaN     NaN
weight         68       83     NaN    NaN  NaN     112
```

If we call `unstack` again, we end up with a `Series` object:
```python
d9 = d8.unstack()
d9
```

**Output:**
```
Paris   alice    birthyear       1985
                 children         NaN
                 hobby         Biking
                 weight            68
        bob      birthyear       1984
                 children         3.0
                 hobby        Dancing
                 weight            83
        charles  birthyear        NaN
                 children         NaN
                 hobby            NaN
                 weight           NaN
London  alice    birthyear        NaN
                 children         NaN
                 hobby            NaN
                 weight           NaN
        bob      birthyear        NaN
                 children         NaN
                 hobby            NaN
                 weight           NaN
        charles  birthyear       1992
                 children         0.0
                 hobby            NaN
                 weight           112
dtype: object
```

## Accessing Rows and Columns

Assuming we have the following `DataFrame`:

```python
people
```

**Output**
```
         weight  birthyear  children    hobby
alice        68       1985       NaN   Biking
bob          83       1984       3.0  Dancing
charles     112       1992       0.0      NaN
```

The columns can be accessed using the `[]` operator.
```python
people["birthyear"]
```
**Output:**
```
alice      1985
bob        1984
charles    1992
Name: birthyear, dtype: int64
```

> **Note**: Equivalently, you can access the columns using attribute-style notation.
> ```python
> people.birthyear
> ```
> **Output:**
> ```
> alice      1985
> bob        1984
> charles    1992
> Name: birthyear, dtype: int64
> ```
> Although this is more convenient, it is not as flexible as the `[]` operator and shouldn't be used for column assignment.

The `loc` attribute lets you access rows instead of columns. The result is a `Series` object in which the `DataFrame`'s column names are mapped to row index labels:

```python
people.iloc[2]
```

**Output**
```
birthyear    1992
hobby         NaN
weight        112
children      0.0
Name: charles, dtype: object
```

The primary methods are `loc` for label-based indexing and `iloc` for integer-based indexing. 

### `loc` and `iloc`

- **`loc`**: Access a group of rows and columns by label(s) or a boolean array.
- **`iloc`**: Purely integer-location based indexing for selection by position.

You can also get a slice of rows, and this returns a `DataFrame` object:
```python
people.iloc[1:3]
```
**
**Output:**
```
         weight  birthyear  children    hobby
bob          83       1984       3.0  Dancing
charles     112       1992       0.0      NaN
```

Using `iloc` and `loc` indexer we can slice the rows and columns of a `DataFrame`.

```python
people.iloc[:3, :2]
```

**Output:**
```
         birthyear    hobby
alice         1985   Biking
bob           1984  Dancing
charles       1992      NaN
```

```python
people.loc[:'bob', 'birthyear']
```

**Output:**
```
       birthyear
alice       1985
bob         1984
```

```python
people.loc[:'bob', 'birthyear':'weight']
```

**Output:**
```
       birthyear    hobby  weight
alice       1985   Biking      68
bob         1984  Dancing      83
```


```python
people[people["birthyear"] < 1990]
```

**Output:**
```
       weight  birthyear  children    hobby
alice      68       1985       NaN   Biking
bob        83       1984       3.0  Dancing
```

The mask can also be used with `loc` indexer.
```python
people.loc[people["birthyear"] < 1990, ["birthyear", "hobby"]]
```

**Output:**
```
       birthyear    hobby
alice       1985   Biking
bob         1984  Dancing
```

### Adding and Removing Columns

```python
people["age"] = 2018 - people["birthyear"]  # adds a new column "age"
people["over 30"] = people["age"] > 30      # adds another column "over 30"
birthyears = people.pop("birthyear")
del people["children"]

people
```

**Output:**
```
         weight    hobby  age  over 30
alice        68   Biking   33     True
bob          83  Dancing   34     True
charles     112      NaN   26    False
```

When you add a new column, it must have the same number of rows. Missing rows are filled with NaN, and extra rows are ignored:
```python
people["pets"] = pd.Series({"bob": 0, "charles": 5, "eugene": 1})  # alice is missing, eugene is ignored
people
```

**Output:**
```
         weight    hobby  age  over 30  pets
alice        68   Biking   33     True   NaN
bob          83  Dancing   34     True   0.0
charles     112      NaN   26    False   5.0
```

<!-- When adding a new column, it is added at the end (on the right) by default. You can also insert a column anywhere else using the `insert()` method:

```python
people.insert(1, "height", [172, 181, 185])
people
```

**Output:**
```
         weight  height    hobby  age  over 30  pets
alice        68     172   Biking   33     True   NaN
bob          83     181  Dancing   34     True   0.0
charles     112     185      NaN   26    False   5.0
``` 

### Adding New Columns
You can also create new columns by calling the `assign()` method. Note that this returns a new `DataFrame` object, the original is not modified:

```python
people.assign(
    body_mass_index = people["weight"] / (people["height"] / 100) ** 2,
    has_pets = people["pets"] > 0
)
```

**Output:**
```
         weight  height    hobby  age  over 30  pets  body_mass_index  \
alice        68     172   Biking   33     True   NaN        22.985398   
bob          83     181  Dancing   34     True   0.0        25.335002   
charles     112     185      NaN   26    False   5.0        32.724617   

         has_pets  
alice       False  
bob         False  
charles      True  
```

You can use chains of `assign()` with lambda functions to create new columns.
```python
(people
     .assign(body_mass_index = lambda df: df["weight"] / (df["height"] / 100) ** 2)
     .assign(overweight = lambda df: df["body_mass_index"] > 25)
)
```
**Output:**
```
         weight  height    hobby  age  over 30  pets  body_mass_index  \
alice        68     172   Biking   33     True   NaN        22.985398   
bob          83     181  Dancing   34     True   0.0        25.335002   
charles     112     185      NaN   26    False   5.0        32.724617   

         overweight  
alice         False  
bob            True  
charles        True 
```

### Evaluating an expression

A great feature supported by pandas is expression evaluation. It relies on the `numexpr` library which must be installed.
```python
people.eval("weight / (height/100) ** 2 > 25")
```

**Output:**
```
alice     False
bob        True
charles    True
dtype: bool
```

You can use the `eval()` method to evaluate an expression and assign the result to a new column.
```python
people.eval("body_mass_index = weight / (height/100) ** 2", inplace=True)
people
``` -->

## Sorting a `DataFrame`
You can sort a `DataFrame` by calling its `sort_index` method. By default, it sorts the rows by their index label, in ascending order, but let's reverse the order:

```python
people.sort_index(ascending=False)
```

**Output:**
```
         weight  birthyear  children    hobby
charles     112       1992       0.0      NaN
bob          83       1984       3.0  Dancing
alice        68       1985       NaN   Biking
```

Note that `sort_index` returned a sorted *copy* of the `DataFrame`. To modify `people` directly, we can set the `inplace` argument to `True`. Also, we can sort the columns instead of the rows by setting `axis=1`:

```python
people.sort_index(axis=1, inplace=True)
people
```

**Output:**
```
         birthyear  children    hobby  weight
alice       1985       NaN   Biking      68
bob         1984       3.0  Dancing      83
charles     1992       0.0      NaN     112
```

To sort the `DataFrame` by the values instead of the labels, we can use `sort_values` and specify the column to sort by:
```python
people.sort_values(by="weight", inplace=True)
people
```

**Output:**
```
         birthyear  children    hobby  weight
alice       1985       NaN   Biking      68
bob         1984       3.0  Dancing      83
charles     1992       0.0      NaN     112
```

## Operations on DataFrames

```python
grades_array = np.array([[8, 8, 9], [10, 9, 9], [4, 8, 2], [9, 10, 10]])
grades = pd.DataFrame(grades_array, columns=["sep", "oct", "nov"], index=["alice", "bob", "charles", "darwin"])
grades
```
**Output:**
```
         sep  oct  nov
alice     8    8    9
bob      10    9    9
charles   4    8    2
darwin    9   10   10
```

You can apply NumPy mathematical functions on a `DataFrame`: the function is applied to all values:

```python
np.sqrt(grades)
```

**Output:**
```
              sep       oct       nov
alice    2.828427  2.828427  3.000000
bob      3.162278  3.000000  3.000000
charles  2.000000  2.828427  1.414214
darwin   3.000000  3.162278  3.162278
```

```python
grades + 1
```

**Output:**
```
         sep  oct  nov
alice      9    9   10
bob       11   10   10
charles    5    9    3
darwin    10   11   11
```

Of course, the same is true for all other binary operations, including arithmetic (`*`,`/`,`**`...) and conditional (`>`, `==`...) operations:

```python
grades >= 5
```

**Output:**
```
         sep  oct  nov
alice    True  True  True
bob      True  True  True
charles  False  True  False
darwin   True  True  True
```

### Simple Aggregation

Aggregation operations, such as computing the `max`, the `sum` or the `mean` of a `DataFrame`, apply to each column, and you get back a `Series` object:

```python
grades.mean()
```

**Output:**
```
sep    7.75
oct    8.75
nov    7.50
dtype: float64
```

## Data Cleaning and Preparation

Data cleaning is the process of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data.

Let's create a new DataFrame for this section with some missing values and duplicates:

```python
import pandas as pd
import numpy as np

data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Alice', 'Eva'],
    'Age': [25, 30, 35, 40, 25, 23],
    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'New York', 'Boston'],
    'Salary': [50000, 60000, 70000, 80000, 50000, np.nan]
}
df = pd.DataFrame(data)
```

### Handling Missing Data

Missing data is a common problem in real-world datasets. Pandas provides several methods to handle missing data, which are represented as `NaN` (Not a Number). 

```python
vals2 = np.array([1, np.nan, 3, 4])
vals2 
# Output: array([1., nan, 3., 4.])
1 + np.nan
# Output: nan
vals2.sum(), vals2.min(), vals2.max()
# Output: (nan, nan, nan)
np.nansum(vals2), np.nanmin(vals2), np.nanmax(vals2)
# Output: (8.0, 1.0, 4.0)
```

In Pandas the `None` value is also considered as missing data but it is not the same as `NaN`. NaN and None both have their place, and Pandas is built to handle the two of them nearly interchangeably, converting between them where appropriate:
```python
pd.Series([1, np.nan, 2, None])
```

**Output:**
```
0    1.0
1    NaN
2    2.0
3    NaN
dtype: float64
```

> **Note:** Pandas Handling of NAs by type
>
> | Type | Conversion when storing NAs | NA sentinel value |
> |------|-----|------|
> | `float` | No Change | `np.nan` |
> | `object` | No Change | `None` or `np.nan` |
> | `int` | Cast to `float64` | `np.nan` |
> | `bool` | Cast to `object` | `None` or `np.nan` |

**Example: Checking for missing data**

```python
# isnull() returns a DataFrame of booleans indicating if a value is missing
print(df.isnull())

# isnull().sum() gives the count of missing values in each column
print(df.isnull().sum())

# .notnull() returns a DataFrame of booleans indicating if a value is not missing
print(df.notnull())
```

**Example: Dropping missing data**

```python
# Drop rows with any missing values
df_dropped = df.dropna()
print(df_dropped)

# Drop columns with any missing values
df_dropped = df.dropna(axis="columns") # or df.dropna(axis=1)
print(df_dropped)

# Drop columns with all missing values
df_dropped = df.dropna(axis="columns", how="all") # or df.dropna(axis=1, how="all")
print(df_dropped)

# Drop rows that have less than 2 non-missing values
df_dropped = df.dropna(axis="rows", thresh=2) # or df.dropna(axis=0, thresh=2)
print(df_dropped)
```

**Example: Filling missing data**

```python
# Fill missing values with 0
df.fillna(0)

# Fill missing values by propagating the previous value forwards
df.fillna(method='ffill')

# Fill missing values by propagating the next value backwards
df.fillna(method='bfill')

# Fill missing salary with the mean salary
mean_salary = df['Salary'].mean()
df_filled = df.fillna({'Salary': mean_salary})
print(df_filled)
```

### Handling Duplicated Data

Duplicate rows can skew analysis. Pandas provides easy ways to find and remove them.

**Example: Identifying duplicate rows**

```python
# The duplicated() method returns a boolean Series indicating duplicate rows
print(df.duplicated())
```

**Example: Dropping duplicate rows**

```python
# The drop_duplicates() method removes duplicate rows
df_no_duplicates = df.drop_duplicates()
print(df_no_duplicates)
```

### Data Type Conversions

Sometimes data is not in the correct format. You can use the `astype()` method to convert data types.

**Example: Converting the 'Age' column to float**

```python
df['Age'] = df['Age'].astype(float)
print(df.dtypes)
```

## Data Transformation

Data transformation is the process of converting data from one format or structure to another. It is a fundamental aspect of most data integration and data management tasks, such as data wrangling and data warehousing.

### Merging, Joining, and Concatenating DataFrames

Pandas provides various facilities for easily combining together Series or DataFrame objects.

- **`concat()`**: Appends one or more DataFrames.
- **`merge()`**: Joins two DataFrames in a database-style join.

#### `concat()`

**Example: Concatenating two Series**
```python
ser1 = pd.Series(['A', 'B', 'C'], index=[1, 2, 3])
ser2 = pd.Series(['D', 'E', 'F'], index=[4, 5, 6])
result = pd.concat([ser1, ser2])
print(result)
```

**Example: Concatenating two DataFrames**

```python
df1 = pd.DataFrame({'A': ['A0', 'A1'], 'B': ['B0', 'B1']})
df2 = pd.DataFrame({'A': ['A2', 'A3'], 'B': ['B2', 'B3']})

result = pd.concat([df1, df2]) # rows i.e. axis=0 by default
print(result)
```

**Output:**
```
    A   B
0  A0  B0
1  A1  B1
0  A2  B2
1  A3  B3
```

```python
result = pd.concat([df1, df2], axis='columns') # or axis=1
print(result)
```

**Output:**
```
    A   B   A   B
0  A0  B0  A2  B2
1  A1  B1  A3  B3
```

```python
result = pd.concat([df1, df2], ignore_index=True) 
print(result)
```

**Output:**
```
   A   B
0  A0  B0
1  A1  B1
2  A2  B2
3  A3  B3
```

```python
result = pd.concat([df1, df2], keys=['X', 'Y']) 
print(result)
```

**Output:**
```
      A   B
X 0  A0  B0
  1  A1  B1
Y 0  A2  B2
  1  A3  B3
```

> **Note:** There is also the `append` method that does similar job.
> ```python
> df1.append(df2)
> ```

#### `merge()`

`merge()` is the function used to join two DataFrames horizontally. A common column is used to align the rows in the two DataFrames.

**Example: Merging two DataFrames**

```python
left = pd.DataFrame({'key': ['K0', 'K1'], 'A': ['A0', 'A1']})
right = pd.DataFrame({'key': ['K0', 'K1'], 'B': ['B0', 'B1']})

result = pd.merge(left, right, on='key')
print(result)
```

**Output:**
```
  key   A   B
0  K0  A0  B0
1  K1  A1  B1
```

#### Categories of Joins

Merging two DataFrames on a common column is a common operation, called join in SQL. Categorically joins can be of the following types:
- One-to-one: Each row in the left DataFrame matches exactly one row in the right DataFrame. This happens when the key columns are unique in both DataFrames.
- One-to-many/many-to-one: Each row in the left DataFrame matches one or more rows in the right DataFrame, and vice versa. This happens when one of the to key columns contains duplicate values.
- Many-to-many: If the key columns are not unique in both DataFrames, the merge will result in a many-to-many join.

**Example: One-to-one join**
```python
df1 = pd.DataFrame({
    'employee': ['Bob', 'Jake', 'Lisa', 'Sue'],
    'group': ['Accounting', 'Engineering','Engineering', 'HR']})
df2 = pd.DataFrame({
    'employee': ['Lisa', 'Bob', 'Jake', 'Sue'],
    'hire_date': [2004, 2008, 2012, 2014]})

df3 = pd.merge(df1, df2, on='employee')
print(df3)
```

**Output:**
```
  employee        group  hire_date
0      Bob   Accounting       2008
1     Jake  Engineering       2012
2     Lisa  Engineering       2004
3      Sue           HR       2014
```

**Example: One-to-many join**
```python
df4 = pd.DataFrame({
    'group': ['Accounting', 'Engineering', 'HR'],
    'supervisor': ['Carly', 'Guido', 'Steve']})
print(pd.merge(df3, df4, on='group'))
```

**Output:**
```
  employee        group  hire_date supervisor
0      Bob   Accounting       2008      Carly
1     Jake  Engineering       2012      Guido
2     Lisa  Engineering       2004      Guido
3      Sue           HR       2014      Steve
```

**Example: Many-to-many join**
```python
df5 = pd.DataFrame({
    'group': ['Accounting', 'Accounting','Engineering', 'Engineering', 'HR', 'HR'],
    'skills': ['math', 'spreadsheets', 'software', 'math', 'spreadsheets', 'organization']})
print(pd.merge(df1, df5, on='group'))
```

**Output:**
```
    employee   group    skills
0   Bob   Accounting    math
1   Bob   Accounting    spreadsheets
2   Jake  Engineering   software
3   Jake  Engineering   math
4   Lisa  Engineering   software
5   Lisa  Engineering   math
6   Sue   HR            spreadsheets
7   Sue   HR            organization
```

#### Merging on Different Keys

When you want to merge two DataFrames on different columns, you can use the `left_on` and `right_on` parameters.
```python
df3 = pd.DataFrame({
    'name': ['Bob', 'Jake', 'Lisa', 'Sue'],
    'salary': [70000, 80000, 120000, 90000]
})
pd.merge(df1, df3, left_on="employee", right_on="name")
```

**Output:**
```
  employee        group  name  salary
0      Bob   Accounting   Bob   70000
1     Jake  Engineering  Jake   80000
2     Lisa  Engineering  Lisa  120000
3      Sue           HR   Sue   90000
```

> **Note:** This process results in a redundant column "name" in the resulting DataFrame. You can drop it using the `.drop('name', axis=1)`.

Sometimes, rather than merging on a column, you would instead like to merge on an index.
```python
df1a = df1.set_index('employee')
df2a = df2.set_index('name')
pd.merge(df1a, df2a, left_index=True, right_index=True)
```
**Output:**
```
                group  hire_date
employee                        
Bob        Accounting       2008
Jake      Engineering       2012
Lisa      Engineering       2004
Sue                HR       2014
```

> **Note:** Pandas includes a method called `join` that is a convenient wrapper for `merge` that defaults to using the index as the key.
> ```python
> df1a.join(df2a)
> ```

#### Set Arithmetic for Joins

What happens when a value appears in one key column but not the other? This is a common problem when merging DataFrames. There are several ways to handle this situation.
- `inner` join: This is the default join type. It returns only the rows that have matching values in both DataFrames.
- `outer` join: This returns all rows from both DataFrames, and fills in the missing values with `NaN`.
- `left` join: This returns all rows from the left DataFrame, and fills in the missing values with `NaN`.
- `right` join: This returns all rows from the right DataFrame, and fills in the missing values with `NaN`.

**Example: Inner join**
```python
df6 = pd.DataFrame({
    'name': ['Peter', 'Paul', 'Mary'],
    'food': ['fish', 'beans', 'bread']},
    columns=['name', 'food'])
df7 = pd.DataFrame({
    'name': ['Mary', 'Joseph'],
    'drink': ['wine', 'beer']},
    columns=['name', 'drink'])
print(pd.merge(df6, df7)) # inner join by default, equivalent to how = 'inner'
```
**Output:**
```
   name   food drink
0  Mary  bread  wine
```

**Example: Outer join**
```python
print(pd.merge(df6, df7, how='outer'))
```
**Output:**
```
     name   food drink
0  Joseph    NaN  beer
1    Mary  bread  wine
2    Paul  beans   NaN
3   Peter   fish   NaN
```

**Example: Left join**
```python
print(pd.merge(df6, df7, how='left'))
```
**Output:**
```
    name   food drink
0  Peter   fish   NaN
1   Paul  beans   NaN
2   Mary  bread  wine
```

**Example: Right join**
```python
print(pd.merge(df6, df7, how='right'))
```
**Output:**
```
     name   food drink
0    Mary  bread  wine
1  Joseph    NaN  beer
```

**Example: Cross join**
```python
print(pd.merge(df6, df7, how='cross'))
```
**Output:**
```
  name_x   food  name_y drink
0  Peter   fish    Mary  wine
1  Peter   fish  Joseph  beer
2   Paul  beans    Mary  wine
3   Paul  beans  Joseph  beer
4   Mary  bread    Mary  wine
5   Mary  bread  Joseph  beer
```

#### Overlapping Column Names

When two DataFrames have overlapping column names, you can use the `prefix` parameter to add a prefix to the overlapping column names.
```python
df8 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],
'rank': [1, 2, 3, 4]})
df9 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],
'rank': [3, 1, 4, 2]})
pd.merge(df8, df9, on="name", suffixes=["_L", "_R"])
```

**Output:**
```
  name  rank_L  rank_R
0   Bob       1       3
1  Jake       2       1
2  Lisa       3       4
3   Sue       4       2
```

### Aggregating and Grouping Data

**Example:**
```python
import seaborn as sns
planets = sns.load_dataset('planets')
print(planets.head())
```

**Output:**
```
            method  number  orbital_period   mass  distance  year
0  Radial Velocity       1         269.300   7.10     77.40  2006
1  Radial Velocity       1         874.774   2.21     56.95  2008
2  Radial Velocity       1         763.000   2.60     19.84  2011
3  Radial Velocity       1         326.030  19.40    110.62  2007
4  Radial Velocity       1         516.220  10.50    119.47  2009
```

Simple aggregation examples are `mean`, `max`, `min`, `sum`, `count`, etc. Most of these methods have a axis parameter that can be set to 0 or 1 (or 'index' or 'columns') to specify the axis along which the operation is performed. By default, axis=0 (row-wise). In addition, there is a convenience method, `describe`, that computes several common aggregates for each column and returns the result.

```python
planets.dropna().describe()
```

**Output:**
```
          number  orbital_period        mass    distance         year
count  498.00000      498.000000  498.000000  498.000000   498.000000
mean     1.73494      835.778671    2.509320   52.068213  2007.377510
std      1.17572     1469.128259    3.636274   46.596041     4.167284
min      1.00000        1.328300    0.003600    1.350000  1989.000000
25%      1.00000       38.272250    0.212500   24.497500  2005.000000
50%      1.00000      357.000000    1.245000   39.940000  2009.000000
75%      2.00000      999.600000    2.867500   59.332500  2011.000000
max      6.00000    17337.500000   25.000000  354.000000  2014.000000
```

Listing of Pandas aggregation methods: 

| **Aggregation** | **Returns** |
| --- | --- |
| `count` | Total number of items |
| `first`, `last` | First and last item |
| `mean`, `median` | Mean and median |
| `min`, `max` | Minimum and maximum |
| `std`, `var` | Standard deviation and variance |
| `mad` | Mean absolute deviation |
| `prod` | Product of all items |
| `sum` | Sum of all items |

The name “group by” comes from a command in the SQL database language, but it is perhaps more illuminative to think of it in the terms first coined by Hadley Wickham of Rstats fame: split, apply, combine.
- The split step involves breaking up and grouping a DataFrame depending on the value of the specified key.
- The apply step involves computing some function, usually an aggregate, transformation, or filtering, within the individual groups.
- The combine step merges the results of these operations into an output array.

![split, apply, combine](images/2.2_split.png)

```python
df = pd.DataFrame({
    'key': ['A', 'B', 'C', 'A', 'B', 'C'],
    'data': range(6)}, 
    columns=['key', 'data'])

print(df)
```

**Output:**
```
  key  data
0   A     0
1   B     1
2   C     2
3   A     3
4   B     4
5   C     5
```

```python
df.groupby('key').sum()
```

**Output:**
```
   data
key    
A     3
B     5
C     7
```

```python
planets.groupby('method')['orbital_period'].median()
```

**Output:**
```
method
Astrometry                      122.0
Imaging                         27500.000000
Microlensing                    3300.000000
Orbital Brightness Modulation   0.342887
Pulsar Timing                   66.541900
Pulsation Timing Variations     1170.000000
Radial Velocity                 360.200000
Transit                         5.714932
Transit Timing Variations       57.011000
Name: orbital_period, dtype: float64
```

The GroupBy object supports direct iteration over the groups, returning each group as a `Series` or `DataFrame`:

```python
for (method, group) in planets.groupby('method'):
    print(f"{method:30s} shape={group.shape}")
```

**Output:**
```
Astrometry                     shape=(2, 6)
Eclipse Timing Variations      shape=(9, 6)
Imaging                        shape=(38, 6)
Microlensing                   shape=(23, 6)
Orbital Brightness Modulation  shape=(3, 6)
Pulsar Timing                  shape=(5, 6)
Pulsation Timing Variations    shape=(1, 6)
Radial Velocity                shape=(553, 6)
Transit                        shape=(397, 6)
Transit Timing Variations      shape=(4, 6)
```

Any method not explicitly implemented by the GroupBy object will be passed through and called on the groups, whether they are DataFrame or Series objects.

```python
planets.groupby('method')['year'].describe()
```

**Output:**
```
                               count         mean       std     min      25%     50%      75%     max
method                                                                                                  
Astrometry                       2.0  2011.500000  2.121320  2010.0  2010.75  2011.5  2012.25  2013.0   
Eclipse Timing Variations        9.0  2010.000000  1.414214  2008.0  2009.00  2010.0  2011.00  2012.0   
Imaging                         38.0  2009.131579  2.781901  2004.0  2008.00  2009.0  2011.00  2013.0   
Microlensing                    23.0  2009.782609  2.859697  2004.0  2008.00  2010.0  2012.00  2013.0   
Orbital Brightness Modulation    3.0  2011.666667  1.154701  2011.0  2011.00  2011.0  2012.00  2013.0   
Pulsar Timing                    5.0  1998.400000  8.384510  1992.0  1992.00  1994.0  2003.00  2011.0   
Pulsation Timing Variations      1.0  2007.000000       NaN  2007.0  2007.00  2007.0  2007.00  2007.0   
Radial Velocity                553.0  2007.518987  4.249052  1989.0  2005.00  2009.0  2011.00  2014.0   
Transit                        397.0  2011.236776  2.077867  2002.0  2010.00  2012.0  2013.00  2014.0   
Transit Timing Variations        4.0  2012.500000  1.290994  2011.0  2011.75  2012.5  2013.25  2014.0 
```

### Aggregate, Filter, Transform, Apply
```python
rng = np.random.RandomState(0)
df = pd.DataFrame({
    'key': ['A', 'B', 'C', 'A', 'B', 'C'],
    'data1': range(6),
    'data2': rng.randint(0, 10, 6)},
    columns = ['key', 'data1', 'data2'])
print(df)
```

**Output:**
```
  key  data1  data2
0   A      0      5
1   B      1      0
2   C      2      3
3   A      3      3
4   B      4      7
5   C      5      9
```

The `aggregate` method can take a string, a function, or a list thereof, and compute all the aggregates at once.

```python
df.groupby('key').aggregate(['min', 'median', 'max'])
```

**Output:**
```
    data1            data2           
      min median max   min median max
key                                  
A       0    1.5   3     3    4.0   5
B       1    2.5   4     0    3.5   7
C       2    3.5   5     3    6.0   9
```

> **Note:** The `agg` method is an alias for `aggregate`.

```python
df.groupby('key').agg({'data1': 'min', 'data2': 'median'})
```

**Output:**
```
    data1  data2
key              
A       0    4.0
B       1    3.5
C       2    6.0
```

Another common use of aggregation is to apply different functions to different columns and rename the output.

```python
df.groupby('key').agg(
    dataX= ('data1', 'min'),
    dataY= ('data2', 'median')
)
```

**Output:**
```
    dataX  dataY
key              
A       0    4.0
B       1    3.5
C       2    6.0
```

A filtering operation allows you to drop data based on the group properties.

```python
def filter_func(x):
    return x['data2'].std() > 4

df.groupby('key').filter(filter_func)
```

**Output:**
```
  key  data1  data2
1   B      1      0
2   C      2      3
4   B      4      7
5   C      5      9
```

While aggregation must return a reduced version of the data, transformation can return some transformed version of the full data to recombine. For such a transformation, the output is the same shape as the input.

```python
def center(x):
    return x - x.mean()
df.groupby('key').transform(center)
```

**Output:**
```
   data1  data2
0   -1.5    1.0
1   -1.5   -3.5
2   -1.5   -3.0
3    1.5   -1.0
4    1.5    3.5
5    1.5    3.0
```

The `apply` method lets you apply an arbitrary function to the group results.

```python
def norm_by_data2(x):
    # x is a DataFrame of group values
    x['data1'] /= x['data2'].sum()
    return x

df.groupby('key')[['data1', 'data2']].apply(norm_by_data2)
```

**Output:**
```
          data1  data2
key                   
A   0  0.000000      5
    3  0.375000      3
B   1  0.142857      0
    4  0.571429      7
C   2  0.166667      3
    5  0.416667      9
```

### Pivot Tables

Pandas provides a `pivot_table` function that can be used to create pivot tables from a DataFrame. Pivoting is a way to transform data by reorganizing it into a tabular format, usually by making one or more variables the index and another variable the column. This is often used to create summary statistics of data.

```python
titanic = sns.load_dataset('titanic')
```

**Output:**
```
   survived  pclass     sex   age  sibsp  parch     fare embarked  class      who  adult_male deck  embark_town alive  alone  
0         0       3    male  22.0      1      0   7.2500        S  Third      man        True  NaN  Southampton    no  False  
1         1       1  female  38.0      1      0  71.2833        C  First    woman       False    C    Cherbourg   yes  False  
2         1       3  female  26.0      0      0   7.9250        S  Third    woman       False  NaN  Southampton   yes   True  
3         1       1  female  35.0      1      0  53.1000        S  First    woman       False    C  Southampton   yes  False  
4         0       3    male  35.0      0      0   8.0500        S  Third      man        True  NaN  Southampton    no   True
```

```python
titanic.groupby('sex')[['survived']].mean()
```

**Output:**
```
        survived
sex             
female  0.742038
male    0.188908
```

```python
titanic.groupby(['sex', 'class'])['survived'].aggregate('mean')
```

**Output:**
```
sex     class 
female  First     0.968085
        Second    0.921053
        Third     0.500000
male    First     0.368852
        Second    0.157407
        Third     0.135447
Name: survived, dtype: float64
```

```python
titanic.groupby(['sex', 'class'])['survived'].aggregate('mean').unstack()
```

**Output:**
```
class      First    Second     Third
sex                                 
female  0.968085  0.921053  0.500000
male    0.368852  0.157407  0.135447
```

Here is the equivalent to the preceding operation using the `DataFrame.pivot_table` method:

```python
titanic.pivot_table('survived', index='sex', columns='class', aggfunc='mean')
```

**Output:**
```
class      First    Second     Third
sex                                 
female  0.968085  0.921053  0.500000
male    0.368852  0.157407  0.135447
```

```python
titanic.pivot_table('survived', index='sex', columns='class', margins=True)
```

**Output:**
```
class      First    Second     Third       All
sex                                           
female  0.968085  0.921053  0.500000  0.742038
male    0.368852  0.157407  0.135447  0.188908
All     0.629630  0.472826  0.242363  0.383838
```

```python
titanic.pivot_table('survived', ['sex', 'sibsp'], 'class')
```

**Output:**
```
class            First    Second     Third
sex    sibsp                              
female 0      0.979592  0.931818  0.592593
       1      0.950000  0.892857  0.447368
       2      1.000000  1.000000  0.571429
       3      1.000000  1.000000  0.125000
       4           NaN       NaN  0.333333
       5           NaN       NaN  0.000000
       8           NaN       NaN  0.000000
male   0      0.329545  0.118421  0.129630
       1      0.483871  0.259259  0.222222
       2      0.500000  0.200000  0.125000
       3      0.000000       NaN  0.000000
       4           NaN       NaN  0.083333
       5           NaN       NaN  0.000000
       8           NaN       NaN  0.000000
```

```python
titanic.pivot_table(index='sex', columns='class', aggfunc={'survived':sum, 'fare':'mean'})
```

**Output:**
```
              fare                          survived             
class        First     Second      Third    First Second Third
sex                                                           
female  106.125798  21.970121  16.118810       91     70    72
male     67.226127  19.741782  12.661633       45     17    47
```

## Vectorized String Operations

```python
monte = pd.Series(['Graham Chapman', 'John Cleese', 'Terry Gilliam', 'Eric Idle', 'Terry Jones', 'Michael Palin'])
monte.str.lower()
```

**Output:**
```
0      graham chapman
1       john cleese
2     terry gilliam
3       eric idle
4      terry jones
5    michael palin
dtype: object
```

```python
monte.str.len()
```

**Output:**
```
0   14
1   11
2   13
3   9
4   11
5   13
dtype: int64
```

```python
monte.str.startswith('T')
```

**Output:**
```
0    False
1    False
2     True
3    False
4     True
5    False
dtype: bool
```

```python
monte.str.split()
```

**Output:**
```
0   [Graham, Chapman]
1   [John, Cleese]
2   [Terry, Gilliam]
3   [Eric, Idle]
4   [Terry, Jones]
5   [Michael, Palin]
dtype: object
```

```python
monte.str[0:3]
```

**Output:**
```
0 Gra
1 Joh
2 Ter
3 Eri
4 Ter
5 Mic
dtype: object
```

```python
monte.str.split().str[-1]
```

**Output:**
```
0 Chapman
1 Cleese
2 Gilliam
3 Idle
4 Jones
5 Palin
dtype: object
```

Python string methods: 
`len` `lower` `translate` `islower` `ljust`
`upper` `startswith` `isupper` `rjust` `find`
`endswith` `isnumeric` `center` `rfind` `isalnum`
`isdecimal` `zfill` `index` `isalpha` `split`
`strip` `rindex` `isdigit` `rsplit` `rstrip`
`capitalize` `isspace` `partition` `lstrip` `swapcase` etc.

## Date/Time Manipulation

Pandas has a rich set of tools for working with dates and times, which is essential for time series analysis.

### Time Series Basics

Python’s basic objects for working with dates and times reside in the built-in datetime module. 

```python
from datetime import datetime
datetime(year=2021, month=7, day=4)
```

```python
datetime.datetime(2021, 7, 4, 0, 0)
```

Once you have a datetime object, you can do things like printing the day of the week:

```python
date.strftime('%A')
```

**Output:**
```
Monday
```

> **Note:** `strftime` is a method of datetime objects that formats the date in a string. These are most common format codes:
> `%Y` - 4-digit year
> `%y` - 2-digit year
> `%m` - 2-digit month
> `%d` - 2-digit day
> `%H` - 2-digit hour (24-hour clock)
> `%M` - 2-digit minute
> `%S` - 2-digit second
> `%A` - full weekday name
> `%a` - abbreviated weekday name
> `%B` - full month name
> `%b` - abbreviated month name

Pandas represents dates and times using the `datetime64` dtype. The `to_datetime()` function can be used to convert strings or other formats to `datetime` objects.

```python
date = pd.to_datetime("4th of July, 2021")
date
```

**Output:**
```
Timestamp('2021-07-04 00:00:00')
```

```python
date.strftime('%A')
```

**Output:**
```
Monday
```

```python
date + pd.to_timedelta(np.arange(12), 'D')
```

**Output:**
```
DatetimeIndex(['2021-07-04', '2021-07-05', '2021-07-06', '2021-07-07',
               '2021-07-08', '2021-07-09', '2021-07-10', '2021-07-11',
               '2021-07-12', '2021-07-13', '2021-07-14', '2021-07-15'],
              dtype='datetime64[ns]', freq=None)
```

### Indexing by Time
```python
index = pd.DatetimeIndex(['2020-07-04', '2020-08-04', '2021-07-04', '2021-08-04'])
data = pd.Series([0, 1, 2, 3], index=index)
data
```

**Output:**
```
2020-07-04    0
2020-08-04    1
2021-07-04    2
2021-08-04    3
dtype: int64
```

And now that we have this data in a Series, we can make use of any of the Series indexing patterns we discussed in previous chapters, passing values that can be coerced into dates:
```python
data['2020-07-04':'2021-07-04']
```

**Output:**
```
2020-07-04 0
2020-08-04 1
2021-07-04 2
dtype: int64
```

```python
data['2021']
```

**Output:**
```
2021-07-04 2
2021-08-04 3
dtype: int64
```

### Pandas Time Series Data Structures

- For timestamps, Pandas provides the `Timestamp` type. As mentioned before, this is essentially a replacement for Python’s native `datetime`, but it’s based on the more efficient `numpy.datetime64` data type. The associated Index structure is `DatetimeIndex`.
- For time periods, Pandas provides the `Period` type. This encodes a fixedfrequency interval based on `numpy.datetime64`. The associated index structure is `PeriodIndex`.
- For time deltas or durations, Pandas provides the `Timedelta` type. `Timedelta` is a more efficient replacement for Python’s native `datetime timedelta` type, and is based on `numpy.timedelta64`. The associated index structure is `TimedeltaIndex`.

| Pandas Type | Description | Based On | Associated Index |
|---|---|---|---|
| `Timestamp` | Replacement for Python's `datetime` | `numpy.datetime64` | `DatetimeIndex` |
| `Period` | Encodes a fixed-frequency interval | `numpy.datetime64` | `PeriodIndex` |
| `Timedelta` | Replacement for Python's `datetime.timedelta` | `numpy.timedelta64` | `TimedeltaIndex` |

```python
dates = pd.to_datetime([datetime(2021, 7, 3), '4th of July, 2021', '2021-Jul-6', '07-07-2021', '20210708'])
dates
```

**Output:**
```
DatetimeIndex(['2021-07-03', '2021-07-04', '2021-07-06', '2021-07-07', '2021-07-08'],
dtype='datetime64[ns]', freq=None)
```

```python
dates.to_period('D')
```

**Output:**
```
PeriodIndex(['2021-07-03', '2021-07-04', '2021-07-06', '2021-07-07', '2021-07-08'],
dtype='period[D]')
```

A `TimedeltaIndex` is created, for example, when a date is subtracted from another:

```python
dates - dates[0]
```

**Output:**
```
TimedeltaIndex(['0 days', '1 days', '3 days', '4 days', '5 days'],
dtype='timedelta64[ns]', freq=None)
```

### Regular Sequences

```python
pd.date_range('2015-07-03', '2015-07-10')
```

**Output:**
```
DatetimeIndex(['2015-07-03', '2015-07-04', '2015-07-05', '2015-07-06', '2015-07-07', '2015-07-08', '2015-07-09', '2015-07-10'],
dtype='datetime64[ns]', freq='D')
```

Alternatively, the date range can be specified not with a start and end point, but with a start point and a number of periods:

```python
pd.date_range('2015-07-03', periods=8)
```

**Output:**
```
DatetimeIndex(['2015-07-03', '2015-07-04', '2015-07-05', '2015-07-06', '2015-07-07', '2015-07-08', '2015-07-09', '2015-07-10'],
dtype='datetime64[ns]', freq='D')
```

The spacing can be modified by altering the freq argument, which defaults to D. For example, here we construct a range of hourly timestamps:

```python
pd.date_range('2015-07-03', periods=8, freq='H')
```

**Output:**
```
DatetimeIndex(['2015-07-03 00:00:00', '2015-07-03 01:00:00',
'2015-07-03 02:00:00', '2015-07-03 03:00:00',
'2015-07-03 04:00:00', '2015-07-03 05:00:00',
'2015-07-03 06:00:00', '2015-07-03 07:00:00'],
dtype='datetime64[ns]', freq='H')
```

To create regular sequences of `Period` or `Timedelta` values, the similar `pd.period_range` and `pd.timedelta_range` functions are useful.

```python
pd.period_range('2015-07', periods=8, freq='M')
```

**Output:**
```
PeriodIndex(['2015-07', '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01', '2016-02'],
dtype='period[M]')
```

```python
pd.timedelta_range(0, periods=6, freq='H')
```

**Output:**
```
TimedeltaIndex(['0 days 00:00:00', '0 days 01:00:00', '0 days 02:00:00', '0 days 03:00:00', '0 days 04:00:00', '0 days 05:00:00'],
dtype='timedelta64[ns]', freq='H')
```

**List of Pandas Frequency Aliases**
| Alias | Description | Business Alias |
|---|---|---|
| `A` | Annual | `BA` - Business year end |
| `Q` | Quarterly | `BQ` - Business quarter end |
| `M` | Monthly | `BM` - Business month end |
| `W` | Weekly | 
| `D` | Calendar daily | `BD` - Business day |
| `H` | Hourly | `BH` - Business hours |
| `T`, `min` | Minutely |
| `S` | Secondly |
| `L`, `ms` | Milliseonds |
| `U`, `us` | Microseconds |
| `N` | Nanoseconds |


**Start-Indexed Frequency Codes**
| Code | Description | Code | Description |
|---|---|---|---|
| `MS` | Month start | `BMS` | Business month start |
| `QS` | Quarter start | `BQS` | Business quarter start |
| `AS` | Year start | `BAS` | Business year start |

```python
pd.timedelta_range(0, periods=6, freq="2H30T")
```

**Output:**
```
TimedeltaIndex(['0 days 00:00:00', '0 days 02:30:00', '0 days 05:00:00', '0 days 07:30:00', '0 days 10:00:00', '0 days 12:30:00'],
dtype='timedelta64[ns]', freq='150T')
```

### Resampling, Shifting, and Windowing
```python
import yfinance as yf
import datetime

tickers = ['NVDA']

start_date = datetime.datetime(2023, 1, 1)
end_date = datetime.datetime.now()
data = yf.download(tickers, start=start_date, end=end_date)
data = data.droplevel(axis=1, level=1)
print(data)
```

**Output:**
```
Price            Close        High         Low        Open     Volume
Date                                                                 
2023-01-03   14.301480   14.981838   14.082687   14.836974  401277000
2023-01-04   14.735069   14.838971   14.227550   14.553242  431324000
2023-01-05   14.251528   14.550245   14.134638   14.477314  389168000
2023-01-06   14.844967   14.995824   14.020746   14.460330  405044000
2023-01-09   15.613240   16.040836   15.126700   15.269565  504231000
...                ...         ...         ...         ...        ...
2025-11-17  186.600006  189.000000  184.320007  185.970001  173628900
2025-11-18  181.360001  184.800003  179.649994  183.380005  213598900
2025-11-19  186.520004  187.860001  182.830002  184.789993  247246400
2025-11-20  180.639999  196.000000  179.850006  195.949997  343504800
2025-11-21  178.880005  184.559998  172.929993  181.240005  346068500

[726 rows x 5 columns]
```

```python
data = data['Close']
```

One common need when dealing with time series data is resampling at a higher or lower frequency. This can be done using the `resample` method, or the much simpler `asfreq` method. The primary difference between the two is that `resample` is fundamentally a data aggregation, while `asfreq` is fundamentally a data selection.

```python
data.resample('BA').mean()
```

**Output:**
```
Date
2023-12-29     36.544537
2024-12-31    107.786020
2025-12-31    150.641728
Freq: BYE-DEC, Name: Close, dtype: float64
```

```python
data.asfreq('BA')
```

**Output:**
```
Date
2023-12-29     49.497181
2024-12-31    134.260757
Freq: BYE-DEC, Name: Close, dtype: float64
```

> **Note:** Notice the difference: at each point, `resample` reports the average of the previous year, while `asfreq` reports the value at the end of the year.

### Time Shifts

```python
data.shift(-365)
```

**Output:**
```
Date
2023-01-03    130.931259
2023-01-04    135.529541
2023-01-05    130.731339
2023-01-06    126.522888
2023-01-09    118.066032
                 ...    
2025-11-17           NaN
2025-11-18           NaN
2025-11-19           NaN
2025-11-20           NaN
2025-11-21           NaN
Name: Close, Length: 726, dtype: float64
```

```python
data.shift(-365) - data
```

**Output:**
```
Date
2023-01-03    116.629780
2023-01-04    120.794471
2023-01-05    116.479813
2023-01-06    111.677921
2023-01-09    102.452792
                 ...    
2025-11-17           NaN
2025-11-18           NaN
2025-11-19           NaN
2025-11-20           NaN
2025-11-21           NaN
Name: Close, Length: 726, dtype: float64
```

### Rolling Windows

```python
data.rolling(window=365).mean()
```

**Output:**
```
Date
2023-01-03    130.931259
2023-01-04    135.529541
2023-01-05    130.731339
2023-01-06    126.522888
2023-01-09    118.066032
                 ...    
2025-11-17           NaN
2025-11-18           NaN
2025-11-19           NaN
2025-11-20           NaN
2025-11-21           NaN
Name: Close, Length: 726, dtype: float64
```

```python
data.rolling(window=365).mean()
```

**Output:**
```
Date
2023-01-03           NaN
2023-01-04           NaN
2023-01-05           NaN
2023-01-06           NaN
2023-01-09           NaN
                 ...    
2025-11-17    141.171006
2025-11-18    141.332583
2025-11-19    141.512246
2025-11-20    141.676101
2025-11-21    141.832663
Name: Close, Length: 726, dtype: float64
```

<!-- TODO: cut, qcut, categorization -->