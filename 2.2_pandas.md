# 2.2. Data Manipulation with Pandas

## Introduction to Pandas

Pandas is a powerful and popular open-source Python library for data manipulation and analysis. It provides high-performance, easy-to-use data structures, and data analysis tools. The name "Pandas" is derived from "Panel Data," an econometrics term for multidimensional, structured data sets.

Pandas is a cornerstone of data science in Python, offering two primary data structures that are fundamental to data manipulation: the `Series` and the `DataFrame`.

## Pandas Data Structures: Series

A **Series** is a one-dimensional labeled array capable of holding any data type (integers, strings, floating-point numbers, Python objects, etc.). It is similar to a column in a spreadsheet or a single column from a database table.

**Example: Creating a Series**

To create a Series, you can pass a list of values to the `pd.Series()` constructor.

```python
import pandas as pd

# Creating a Series from a list
data = [2,-1,3,5]
s = pd.Series(data)
print(s)
```

**Output:**

```
0    2
1   -1
2    3
3    5
dtype: int64
```

`Series` objects behave much like one-dimensional NumPy `ndarray`s, and you can often pass them as parameters to NumPy functions:
```python
import numpy as np
np.exp(s)
```

**Output:**

```
0      7.389056
1      0.367879
2     20.085537
3    148.413159
dtype: float64
```

Arithmetic operations on `Series` are also possible, and they apply *elementwise*, just like for `ndarray`s:
```python
s + [1000,2000,3000,4000]
```

**Output:**

```
0    1002
1    1999
2    3003
3    4005
dtype: int64
```

Similar to NumPy, if you add a single number to a `Series`, that number is added to all items in the `Series`. This is called *broadcasting*:

```python
s + 1000
```

**Output:**

```
0    1002
1     999
2    1003
3    1005
dtype: int64
```

The same is true for all binary operations such as `*` or `/`, and even conditional operations:

```python
s < 0
```

**Output:**

```
0    False
1     True
2    False
3    False
dtype: bool
```

### Index
By default, index is simply the rank of the item in the `Series` (starting from `0`) but you can also set the index labels manually:

```python
s2 = pd.Series(
    [68, 83, 112, 68], 
    index=["alice", "bob", "charles", "darwin"]
)
s2
```

**Output:**

```
alice       68
bob         83
charles    112
darwin      68
dtype: int64
```

> **Note:** You can also access the index of the `Series` using the `index` attribute.
> s2.index
> 
> **Output:**
> ```
> Index(['alice', 'bob', 'charles', 'darwin'], dtype='object')
> ```

> **Note:** If you initialize a `Series` with a scalar value and a list of indices, the scalar value is repeated for each index.
>
> ```python
> pd.Series(42, ["life", "universe", "everything"])
> ```
>
> **Output:**
>
> ```
> life          42
> universe      42
> everything    42
> dtype: int64
> ```

You can then use the `Series` just like a `dict`:

```python
s2["bob"]
```

**Output:**

```
83
```

You can also use the `Series` as a dictionary to set values:

```python
s2["bob"] = 100
print(s2)
```

To make it clear when you are accessing by label or by integer location, it is recommended to always use the `loc` attribute when accessing by label, and the `iloc` attribute when accessing by integer location:

```python
s2.loc["bob"]
```

**Output:**

```
83
```

```python
s2.iloc[1]
```

**Output:**

```
83
```
Slicing a Series is similar to slicing a list:

```python
s2.iloc[1:3]
```

**Output:**

```
bob         83
charles    112
dtype: int64
```

### Alternative Constructor

You can also create a `Series` from a dictionary:

```python
weights = {"alice": 68, "bob": 83, "colin": 86, "darwin": 68}
s3 = pd.Series(weights)
s3
```

**Output:**

```
alice     68
bob       83
colin     86
darwin    68
dtype: int64
```

### Automatic Alignment

When you perform arithmetic operations on `Series`, the operations are performed element-wise, and the indices of the `Series` are automatically aligned.

```python
s3 + s2
```

**Output:**

```
alice      136.0
bob        166.0
charles      NaN
colin        NaN
darwin     136.0
dtype: float64
```

> **Note:** NaN stands for "Not a Number" and is used to represent missing values.

### Name of the Series

You can also set the name of the `Series` using the `name` parameter:

```python
s6 = pd.Series([83, 68], index=["bob", "alice"], name="weights")
s6
```

**Output:**

```
bob      83
alice    68
Name: weights, dtype: int64
```

## Pandas Data Structures: DataFrame

A **DataFrame** is a two-dimensional labeled data structure with columns of potentially different types. It is similar to a spreadsheet, a SQL table, or a dictionary of Series objects. It is the most commonly used pandas object.

## Creating a DataFrame

You can create a DataFrame from a dictionary of lists, where each list represents a column.

```python
import pandas as pd

# Creating a DataFrame from a dictionary
data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'David'],
    'Age': [25, 30, 35, 40],
    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston']
}
df = pd.DataFrame(data)
print(df)
```

**Output:**

```
      Name  Age         City
0    Alice   25     New York
1      Bob   30  Los Angeles
2  Charlie   35      Chicago
3    David   40      Houston
```

You can also create a DataFrame from a dictionary of `Series` objects:

```python
weight_series = pd.Series([68, 83, 112], index=["alice", "bob", "charles"])
birthyear_series = pd.Series([1984, 1985, 1992], index=["bob", "alice", "charles"], name="year")
children_series = pd.Series([0, 3], index=["charles", "bob"])
hobby_series = pd.Series(["Biking", "Dancing"], index=["alice", "bob"])

people_dict = {
    "weight": weight_series,
    "birthyear": birthyear_series,
    "children": children_series,
    "hobby": hobby_series,
}
people = pd.DataFrame(people_dict)
people
```

**Output:**

```
         weight  birthyear  children    hobby
alice        68       1985       NaN   Biking
bob          83       1984       3.0  Dancing
charles     112       1992       0.0      NaN
```

> **Note:** The `Series` were automatically aligned based on the indices and the missing values are represented as `NaN`. Also note that the name `year` was dropped from the `birthyear` `Series`.

> **Note:** The index of the `DataFrame` can be accessed using the `index` attribute.
> ```python
> people.index
> ```
> **Output:**
> ```
> Index(['alice', 'bob', 'charles'], dtype='object')
> ```
> ```python
> people.columns
> ```
> **Output:**
> ```
> Index(['weight', 'birthyear', 'children', 'hobby'], dtype='object')
> ```

You can access columns using the `[]` operator. They are returned as `Series` objects:

```python
people["birthyear"]
```

**Output:**

```
alice      1985
bob        1984
charles    1992
Name: birthyear, dtype: int64
```

You can also get multiple columns at once:
```python
people[["birthyear", "hobby"]]
```

**Output:**

```
         birthyear    hobby
alice       1985   Biking
bob         1984  Dancing
charles     1992      NaN
dtype: object
```

If you pass a list of columns and/or index row labels to the `DataFrame` constructor.

```python
d2 = pd.DataFrame(
        people_dict,
        columns=["birthyear", "weight", "height"], # reorder the columns
        index=["bob", "alice", "eugene"] # reorder the index
     )
d2
```
**Output:**

```
        birthyear  weight height
bob        1984.0    83.0    NaN
alice      1985.0    68.0    NaN
eugene        NaN     NaN    NaN
```

### Alternative Constructors

Another convenient way to create a `DataFrame` is to pass all the values to the constructor as an `ndarray`, or a list of lists, and specify the column names and row index labels separately:

```python
values = [
            [1985, np.nan, "Biking",   68],
            [1984, 3,      "Dancing",  83],
            [1992, 0,      np.nan,    112]
         ]
d3 = pd.DataFrame(
        values,
        columns=["birthyear", "children", "hobby", "weight"],
        index=["alice", "bob", "charles"]
     )
d3
```

**Output:**

```
         birthyear  children    hobby  weight
alice         1985       NaN   Biking      68
bob           1984       3.0  Dancing      83
charles       1992       0.0      NaN     112
```

It is also possible to create a `DataFrame` with a dictionary (or list) of dictionaries (or lists):

```python
people = pd.DataFrame({
    "birthyear": {"alice": 1985, "bob": 1984, "charles": 1992},
    "hobby": {"alice": "Biking", "bob": "Dancing"},
    "weight": {"alice": 68, "bob": 83, "charles": 112},
    "children": {"bob": 3, "charles": 0}
})
people
```

**Output:**

```
         birthyear    hobby  weight  children
alice         1985   Biking      68       NaN
bob           1984  Dancing      83       3.0
charles       1992      NaN     112       0.0
```

### Multi-Index DataFrame
```python
d5 = pd.DataFrame(
  {
    ("public", "birthyear"):
        {("Paris","alice"): 1985, ("Paris","bob"): 1984, ("London","charles"): 1992},
    ("public", "hobby"):
        {("Paris","alice"): "Biking", ("Paris","bob"): "Dancing"},
    ("private", "weight"):
        {("Paris","alice"): 68, ("Paris","bob"): 83, ("London","charles"): 112},
    ("private", "children"):
        {("Paris", "alice"): np.nan, ("Paris","bob"): 3, ("London","charles"): 0}
  }
)
d5
```

**Output:**

```
                public              private         
                birthyear    hobby  weight children
Paris  alice        1985   Biking      68      NaN
       bob          1984  Dancing      83      3.0
London charles      1992      NaN     112      0.0
```

You can now get a `DataFrame` containing all the `"public"` columns very simply:
```python
d5["public"]
```

**Output:**

```
                birthyear    hobby
Paris  alice         1985   Biking
       bob           1984  Dancing
London charles       1992      NaN
```

```python
d5["public", "hobby"]  # Same result as d5["public"]["hobby"]
```

**Output:**

```
Paris   alice       Biking
        bob        Dancing
London  charles        NaN
Name: (public, hobby), dtype: object
```

> **Note:** Multi-Index DataFrame can also be created using the `MultiIndex` class.
> ```python
> columns = pd.MultiIndex.from_tuples([('public', 'birthyear'), ('public', 'hobby'), ('private', 'weight'), ('private', 'children')])
> index = pd.MultiIndex.from_tuples([('Paris', 'alice'), ('Paris', 'bob'), ('London', 'charles')])
> data = [
>     [1985, 'Biking', 68, 'NaN'],
>     [1984, 'Dancing', 83, 3.0],
>     [1992, 'NaN', 112, 0.0]
>     ]
> d5 = pd.DataFrame(data, index=index, columns=columns)
> d5
> ```
> **Output:**
> ```
>                   public              private         
>                   birthyear    hobby  weight children
> Paris  alice        1985   Biking      68      NaN
>        bob          1984  Dancing      83      3.0
> London charles      1992      NaN     112      0.0
> ```
> Or even simpler:
> ```python
> d5 = pd.DataFrame(data, index=[['Paris', 'Paris', 'London'], ['alice', 'bob', 'charles']], columns=[['public', 'public', 'private', 'private'], ['birthyear', 'hobby', 'weight', 'children']])
> d5
> ```
> **Output:**
> ```
>                   public              private         
>                   birthyear    hobby  weight children
> Paris  alice        1985   Biking      68      NaN
>        bob          1984  Dancing      83      3.0
> London charles      1992      NaN     112      0.0
> ```

There are two levels of columns, and two levels of indices. We can drop a column level by calling `droplevel()` (the same goes for indices):

```python
d5.columns = d5.columns.droplevel(level = 0)
d5
```

**Output:**

```
                birthyear    hobby  weight  children
Paris  alice         1985   Biking      68       NaN
       bob           1984  Dancing      83       3.0
London charles       1992      NaN     112       0.0
```

```python
d6 = d5.T
d6
```

**Output:**

```
            Paris           London
            alice      bob charles
birthyear    1985     1984    1992
hobby      Biking  Dancing     NaN
weight         68       83     112
children      NaN      3.0     0.0
```

### Stacking and Unstacking

Calling the `stack()` method will push the lowest column level after the lowest index:

```python
d7 = d6.stack()
d7
```

**Output:**

```python
                     Paris London
birthyear alice       1985    NaN
          bob         1984    NaN
          charles      NaN   1992
hobby     alice     Biking    NaN
          bob      Dancing    NaN
weight    alice         68    NaN
          bob           83    NaN
          charles      NaN    112
children  bob          3.0    NaN
          charles      NaN    0.0
```

Calling `unstack()` will do the reverse, once again creating many `NaN` values.

> **Note:** Many `NaN` values appeared. This is because many new combinations did not exist before.

```python
d8 = d7.unstack()
d8
```

**Output:**
```
            Paris                  London             
            alice      bob charles  alice  bob charles
birthyear    1985     1984     NaN    NaN  NaN    1992
children      NaN      3.0     NaN    NaN  NaN     0.0
hobby      Biking  Dancing     NaN    NaN  NaN     NaN
weight         68       83     NaN    NaN  NaN     112
```

If we call `unstack` again, we end up with a `Series` object:
```python
d9 = d8.unstack()
d9
```

**Output:**
```
Paris   alice    birthyear       1985
                 children         NaN
                 hobby         Biking
                 weight            68
        bob      birthyear       1984
                 children         3.0
                 hobby        Dancing
                 weight            83
        charles  birthyear        NaN
                 children         NaN
                 hobby            NaN
                 weight           NaN
London  alice    birthyear        NaN
                 children         NaN
                 hobby            NaN
                 weight           NaN
        bob      birthyear        NaN
                 children         NaN
                 hobby            NaN
                 weight           NaN
        charles  birthyear       1992
                 children         0.0
                 hobby            NaN
                 weight           112
dtype: object
```

## Accessing Rows and Columns

Assuming we have the following `DataFrame`:

```python
people
```

**Output**
```
         weight  birthyear  children    hobby
alice        68       1985       NaN   Biking
bob          83       1984       3.0  Dancing
charles     112       1992       0.0      NaN
```

The columns can be accessed using the `[]` operator.
```python
people["birthyear"]
```
**Output:**
```
alice      1985
bob        1984
charles    1992
Name: birthyear, dtype: int64
```

> **Note**: Equivalently, you can access the columns using attribute-style notation.
> ```python
> people.birthyear
> ```
> **Output:**
> ```
> alice      1985
> bob        1984
> charles    1992
> Name: birthyear, dtype: int64
> ```
> Although this is more convenient, it is not as flexible as the `[]` operator and shouldn't be used for column assignment.

The `loc` attribute lets you access rows instead of columns. The result is a `Series` object in which the `DataFrame`'s column names are mapped to row index labels:

```python
people.iloc[2]
```

**Output**
```
birthyear    1992
hobby         NaN
weight        112
children      0.0
Name: charles, dtype: object
```

You can also get a slice of rows, and this returns a `DataFrame` object:
```python
people.iloc[1:3]
```
**
**Output:**
```
         weight  birthyear  children    hobby
bob          83       1984       3.0  Dancing
charles     112       1992       0.0      NaN
```

Using `iloc` and `loc` indexer we can slice the rows and columns of a `DataFrame`.

```python
people.iloc[:3, :2]
```

**Output:**
```
         birthyear    hobby
alice         1985   Biking
bob           1984  Dancing
charles       1992      NaN
```

```python
people.loc[:'bob', 'birthyear':'weight']
```

**Output:**
```
       birthyear    hobby  weight
alice       1985   Biking      68
bob         1984  Dancing      83
```


```python
people[people["birthyear"] < 1990]
```

**Output:**
```
       weight  birthyear  children    hobby
alice      68       1985       NaN   Biking
bob        83       1984       3.0  Dancing
```

The mask can also be used with `loc` indexer.
```python
people.loc[people["birthyear"] < 1990, ["birthyear", "hobby"]]
```

**Output:**
```
       birthyear    hobby
alice       1985   Biking
bob         1984  Dancing
```

### Adding and Removing Columns

```python
people["age"] = 2018 - people["birthyear"]  # adds a new column "age"
people["over 30"] = people["age"] > 30      # adds another column "over 30"
birthyears = people.pop("birthyear")
del people["children"]

people
```

**Output:**
```
         weight    hobby  age  over 30
alice        68   Biking   33     True
bob          83  Dancing   34     True
charles     112      NaN   26    False
```

When you add a new column, it must have the same number of rows. Missing rows are filled with NaN, and extra rows are ignored:
```python
people["pets"] = pd.Series({"bob": 0, "charles": 5, "eugene": 1})  # alice is missing, eugene is ignored
people
```

**Output:**
```
         weight    hobby  age  over 30  pets
alice        68   Biking   33     True   NaN
bob          83  Dancing   34     True   0.0
charles     112      NaN   26    False   5.0
```

<!-- When adding a new column, it is added at the end (on the right) by default. You can also insert a column anywhere else using the `insert()` method:

```python
people.insert(1, "height", [172, 181, 185])
people
```

**Output:**
```
         weight  height    hobby  age  over 30  pets
alice        68     172   Biking   33     True   NaN
bob          83     181  Dancing   34     True   0.0
charles     112     185      NaN   26    False   5.0
``` 

### Adding New Columns
You can also create new columns by calling the `assign()` method. Note that this returns a new `DataFrame` object, the original is not modified:

```python
people.assign(
    body_mass_index = people["weight"] / (people["height"] / 100) ** 2,
    has_pets = people["pets"] > 0
)
```

**Output:**
```
         weight  height    hobby  age  over 30  pets  body_mass_index  \
alice        68     172   Biking   33     True   NaN        22.985398   
bob          83     181  Dancing   34     True   0.0        25.335002   
charles     112     185      NaN   26    False   5.0        32.724617   

         has_pets  
alice       False  
bob         False  
charles      True  
```

You can use chains of `assign()` with lambda functions to create new columns.
```python
(people
     .assign(body_mass_index = lambda df: df["weight"] / (df["height"] / 100) ** 2)
     .assign(overweight = lambda df: df["body_mass_index"] > 25)
)
```
**Output:**
```
         weight  height    hobby  age  over 30  pets  body_mass_index  \
alice        68     172   Biking   33     True   NaN        22.985398   
bob          83     181  Dancing   34     True   0.0        25.335002   
charles     112     185      NaN   26    False   5.0        32.724617   

         overweight  
alice         False  
bob            True  
charles        True 
```

### Evaluating an expression

A great feature supported by pandas is expression evaluation. It relies on the `numexpr` library which must be installed.
```python
people.eval("weight / (height/100) ** 2 > 25")
```

**Output:**
```
alice     False
bob        True
charles    True
dtype: bool
```

You can use the `eval()` method to evaluate an expression and assign the result to a new column.
```python
people.eval("body_mass_index = weight / (height/100) ** 2", inplace=True)
people
``` -->

## Sorting a `DataFrame`
You can sort a `DataFrame` by calling its `sort_index` method. By default, it sorts the rows by their index label, in ascending order, but let's reverse the order:

```python
people.sort_index(ascending=False)
```

**Output:**
```
         weight  birthyear  children    hobby
charles     112       1992       0.0      NaN
bob          83       1984       3.0  Dancing
alice        68       1985       NaN   Biking
```

Note that `sort_index` returned a sorted *copy* of the `DataFrame`. To modify `people` directly, we can set the `inplace` argument to `True`. Also, we can sort the columns instead of the rows by setting `axis=1`:

```python
people.sort_index(axis=1, inplace=True)
people
```

**Output:**
```
         birthyear  children    hobby  weight
alice       1985       NaN   Biking      68
bob         1984       3.0  Dancing      83
charles     1992       0.0      NaN     112
```

To sort the `DataFrame` by the values instead of the labels, we can use `sort_values` and specify the column to sort by:
```python
people.sort_values(by="weight", inplace=True)
people
```

**Output:**
```
         birthyear  children    hobby  weight
alice       1985       NaN   Biking      68
bob         1984       3.0  Dancing      83
charles     1992       0.0      NaN     112
```

## Operations on DataFrames

```python
grades_array = np.array([[8, 8, 9], [10, 9, 9], [4, 8, 2], [9, 10, 10]])
grades = pd.DataFrame(grades_array, columns=["sep", "oct", "nov"], index=["alice", "bob", "charles", "darwin"])
grades
```
**Output:**
```
         sep  oct  nov
alice     8    8    9
bob      10    9    9
charles   4    8    2
darwin    9   10   10
```

You can apply NumPy mathematical functions on a `DataFrame`: the function is applied to all values:

```python
np.sqrt(grades)
```

**Output:**
```
              sep       oct       nov
alice    2.828427  2.828427  3.000000
bob      3.162278  3.000000  3.000000
charles  2.000000  2.828427  1.414214
darwin   3.000000  3.162278  3.162278
```

```python
grades + 1
```

**Output:**
```
         sep  oct  nov
alice      9    9   10
bob       11   10   10
charles    5    9    3
darwin    10   11   11
```

Of course, the same is true for all other binary operations, including arithmetic (`*`,`/`,`**`...) and conditional (`>`, `==`...) operations:

```python
grades >= 5
```

**Output:**
```
         sep  oct  nov
alice    True  True  True
bob      True  True  True
charles  False  True  False
darwin   True  True  True
```

Aggregation operations, such as computing the `max`, the `sum` or the `mean` of a `DataFrame`, apply to each column, and you get back a `Series` object:

```python
grades.mean()
```

**Output:**
```
sep    7.75
oct    8.75
nov    7.50
dtype: float64
```

## Data Cleaning and Preparation

Data cleaning is the process of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data.

Let's create a new DataFrame for this section with some missing values and duplicates:

```python
import pandas as pd
import numpy as np

data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Alice', 'Eva'],
    'Age': [25, 30, 35, 40, 25, 23],
    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'New York', 'Boston'],
    'Salary': [50000, 60000, 70000, 80000, 50000, np.nan]
}
df = pd.DataFrame(data)
```

### Handling Missing Data

Missing data is a common problem in real-world datasets. Pandas provides several methods to handle missing data, which are represented as `NaN` (Not a Number). 

```python
vals2 = np.array([1, np.nan, 3, 4])
vals2 
# Output: array([1., nan, 3., 4.])
1 + np.nan
# Output: nan
vals2.sum(), vals2.min(), vals2.max()
# Output: (nan, nan, nan)
np.nansum(vals2), np.nanmin(vals2), np.nanmax(vals2)
# Output: (8.0, 1.0, 4.0)
```

In Pandas the `None` value is also considered as missing data but it is not the same as `NaN`. NaN and None both have their place, and Pandas is built to handle the two of them nearly interchangeably, converting between them where appropriate:
```python
pd.Series([1, np.nan, 2, None])
```

**Output:**
```
0    1.0
1    NaN
2    2.0
3    NaN
dtype: float64
```

> **Note:** Pandas Handling of NAs by type
>
> | Type | Conversion when storing NAs | NA sentinel value |
> |------|-----|------|
> | `float` | No Change | `np.nan` |
> | `object` | No Change | `None` or `np.nan` |
> | `int` | Cast to `float64` | `np.nan` |
> | `bool` | Cast to `object` | `None` or `np.nan` |

**Example: Checking for missing data**

```python
# isnull() returns a DataFrame of booleans indicating if a value is missing
print(df.isnull())

# isnull().sum() gives the count of missing values in each column
print(df.isnull().sum())

# .notnull() returns a DataFrame of booleans indicating if a value is not missing
print(df.notnull())
```

**Example: Dropping missing data**

```python
# Drop rows with any missing values
df_dropped = df.dropna()
print(df_dropped)

# Drop columns with any missing values
df_dropped = df.dropna(axis="columns") # or df.dropna(axis=1)
print(df_dropped)

# Drop columns with all missing values
df_dropped = df.dropna(axis="columns", how="all") # or df.dropna(axis=1, how="all")
print(df_dropped)

# Drop rows that have less than 2 non-missing values
df_dropped = df.dropna(axis="rows", thresh=2) # or df.dropna(axis=0, thresh=2)
print(df_dropped)
```

**Example: Filling missing data**

```python
# Fill missing values with 0
df.fillna(0)

# Fill missing values by propagating the previous value forwards
df.fillna(method='ffill')

# Fill missing values by propagating the next value backwards
df.fillna(method='bfill')

# Fill missing salary with the mean salary
mean_salary = df['Salary'].mean()
df_filled = df.fillna({'Salary': mean_salary})
print(df_filled)
```

### Handling Duplicated Data

Duplicate rows can skew analysis. Pandas provides easy ways to find and remove them.

**Example: Identifying duplicate rows**

```python
# The duplicated() method returns a boolean Series indicating duplicate rows
print(df.duplicated())
```

**Example: Dropping duplicate rows**

```python
# The drop_duplicates() method removes duplicate rows
df_no_duplicates = df.drop_duplicates()
print(df_no_duplicates)
```

### Data Type Conversions

Sometimes data is not in the correct format. You can use the `astype()` method to convert data types.

**Example: Converting the 'Age' column to float**

```python
df['Age'] = df['Age'].astype(float)
print(df.dtypes)
```

## Data Transformation

Data transformation is the process of converting data from one format or structure to another. It is a fundamental aspect of most data integration and data management tasks, such as data wrangling and data warehousing.

### Merging, Joining, and Concatenating DataFrames

Pandas provides various facilities for easily combining together Series or DataFrame objects.

- **`concat()`**: Appends one or more DataFrames.
- **`merge()`**: Joins two DataFrames in a database-style join.

#### `concat()`

**Example: Concatenating two Series**
```python
ser1 = pd.Series(['A', 'B', 'C'], index=[1, 2, 3])
ser2 = pd.Series(['D', 'E', 'F'], index=[4, 5, 6])
result = pd.concat([ser1, ser2])
print(result)
```

**Example: Concatenating two DataFrames**

```python
df1 = pd.DataFrame({'A': ['A0', 'A1'], 'B': ['B0', 'B1']})
df2 = pd.DataFrame({'A': ['A2', 'A3'], 'B': ['B2', 'B3']})

result = pd.concat([df1, df2]) # rows i.e. axis=0 by default
print(result)
```

**Output:**
```
    A   B
0  A0  B0
1  A1  B1
0  A2  B2
1  A3  B3
```

```python
result = pd.concat([df1, df2], axis='columns') # or axis=1
print(result)
```

**Output:**
```
    A   B   A   B
0  A0  B0  A2  B2
1  A1  B1  A3  B3
```

```python
result = pd.concat([df1, df2], ignore_index=True) 
print(result)
```

**Output:**
```
   A   B
0  A0  B0
1  A1  B1
2  A2  B2
3  A3  B3
```

```python
result = pd.concat([df1, df2], keys=['X', 'Y']) 
print(result)
```

**Output:**
```
      A   B
X 0  A0  B0
  1  A1  B1
Y 0  A2  B2
  1  A3  B3
```

> **Note:** There is also the `append` method that does similar job.
> ```python
> df1.append(df2)
> ```

#### `merge()`

`merge()` is the function used to join two DataFrames horizontally. A common column is used to align the rows in the two DataFrames.

**Example: Merging two DataFrames**

```python
left = pd.DataFrame({'key': ['K0', 'K1'], 'A': ['A0', 'A1']})
right = pd.DataFrame({'key': ['K0', 'K1'], 'B': ['B0', 'B1']})

result = pd.merge(left, right, on='key')
print(result)
```

**Output:**
```
  key   A   B
0  K0  A0  B0
1  K1  A1  B1
```

#### Categories of Joins

Merging two DataFrames on a common column is a common operation, called join in SQL. Categorically joins can be of the following types:
- One-to-one: Each row in the left DataFrame matches exactly one row in the right DataFrame. This happens when the key columns are unique in both DataFrames.
- One-to-many/many-to-one: Each row in the left DataFrame matches one or more rows in the right DataFrame, and vice versa. This happens when one of the to key columns contains duplicate values.
- Many-to-many: If the key columns are not unique in both DataFrames, the merge will result in a many-to-many join.

**Example: One-to-one join**
```python
df1 = pd.DataFrame({
    'employee': ['Bob', 'Jake', 'Lisa', 'Sue'],
    'group': ['Accounting', 'Engineering','Engineering', 'HR']})
df2 = pd.DataFrame({
    'employee': ['Lisa', 'Bob', 'Jake', 'Sue'],
    'hire_date': [2004, 2008, 2012, 2014]})

df3 = pd.merge(df1, df2, on='employee')
print(df3)
```

**Output:**
```
  employee        group  hire_date
0      Bob   Accounting       2008
1     Jake  Engineering       2012
2     Lisa  Engineering       2004
3      Sue           HR       2014
```

**Example: One-to-many join**
```python
df4 = pd.DataFrame({
    'group': ['Accounting', 'Engineering', 'HR'],
    'supervisor': ['Carly', 'Guido', 'Steve']})
print(pd.merge(df3, df4, on='group'))
```

**Output:**
```
  employee        group  hire_date supervisor
0      Bob   Accounting       2008      Carly
1     Jake  Engineering       2012      Guido
2     Lisa  Engineering       2004      Guido
3      Sue           HR       2014      Steve
```

**Example: Many-to-many join**
```python
df5 = pd.DataFrame({
    'group': ['Accounting', 'Accounting','Engineering', 'Engineering', 'HR', 'HR'],
    'skills': ['math', 'spreadsheets', 'software', 'math', 'spreadsheets', 'organization']})
print(pd.merge(df1, df5, on='group'))
```

**Output:**
```
    employee   group    skills
0   Bob   Accounting    math
1   Bob   Accounting    spreadsheets
2   Jake  Engineering   software
3   Jake  Engineering   math
4   Lisa  Engineering   software
5   Lisa  Engineering   math
6   Sue   HR            spreadsheets
7   Sue   HR            organization
```

#### Merging on Different Keys

When you want to merge two DataFrames on different columns, you can use the `left_on` and `right_on` parameters.
```python
df3 = pd.DataFrame({
    'name': ['Bob', 'Jake', 'Lisa', 'Sue'],
    'salary': [70000, 80000, 120000, 90000]
})
pd.merge(df1, df3, left_on="employee", right_on="name")
```

**Output:**
```
  employee        group  name  salary
0      Bob   Accounting   Bob   70000
1     Jake  Engineering  Jake   80000
2     Lisa  Engineering  Lisa  120000
3      Sue           HR   Sue   90000
```

> **Note:** This process results in a redundant column "name" in the resulting DataFrame. You can drop it using the `.drop('name', axis=1)`.

Sometimes, rather than merging on a column, you would instead like to merge on an index.
```python
df1a = df1.set_index('employee')
df2a = df2.set_index('name')
pd.merge(df1a, df2a, left_index=True, right_index=True)
```
**Output:**
```
                group  hire_date
employee                        
Bob        Accounting       2008
Jake      Engineering       2012
Lisa      Engineering       2004
Sue                HR       2014
```

> **Note:** Pandas includes a method called `join` that is a convenient wrapper for `merge` that defaults to using the index as the key.
> ```python
> df1a.join(df2a)
> ```

#### Set Arithmetic for Joins

What happens when a value appears in one key column but not the other? This is a common problem when merging DataFrames. There are several ways to handle this situation.
- `inner` join: This is the default join type. It returns only the rows that have matching values in both DataFrames.
- `outer` join: This returns all rows from both DataFrames, and fills in the missing values with `NaN`.
- `left` join: This returns all rows from the left DataFrame, and fills in the missing values with `NaN`.
- `right` join: This returns all rows from the right DataFrame, and fills in the missing values with `NaN`.

**Example: Inner join**
```python
df6 = pd.DataFrame({
    'name': ['Peter', 'Paul', 'Mary'],
    'food': ['fish', 'beans', 'bread']},
    columns=['name', 'food'])
df7 = pd.DataFrame({
    'name': ['Mary', 'Joseph'],
    'drink': ['wine', 'beer']},
    columns=['name', 'drink'])
print(pd.merge(df6, df7)) # inner join by default, equivalent to how = 'inner'
```
**Output:**
```
   name   food drink
0  Mary  bread  wine
```

**Example: Outer join**
```python
print(pd.merge(df6, df7, how='outer'))
```
**Output:**
```
     name   food drink
0  Joseph    NaN  beer
1    Mary  bread  wine
2    Paul  beans   NaN
3   Peter   fish   NaN
```

**Example: Left join**
```python
print(pd.merge(df6, df7, how='left'))
```
**Output:**
```
    name   food drink
0  Peter   fish   NaN
1   Paul  beans   NaN
2   Mary  bread  wine
```

**Example: Right join**
```python
print(pd.merge(df6, df7, how='right'))
```
**Output:**
```
     name   food drink
0    Mary  bread  wine
1  Joseph    NaN  beer
```

**Example: Cross join**
```python
print(pd.merge(df6, df7, how='cross'))
```
**Output:**
```
  name_x   food  name_y drink
0  Peter   fish    Mary  wine
1  Peter   fish  Joseph  beer
2   Paul  beans    Mary  wine
3   Paul  beans  Joseph  beer
4   Mary  bread    Mary  wine
5   Mary  bread  Joseph  beer
```

#### Overlapping Column Names

When two DataFrames have overlapping column names, you can use the `prefix` parameter to add a prefix to the overlapping column names.
```python
df8 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],
'rank': [1, 2, 3, 4]})
df9 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],
'rank': [3, 1, 4, 2]})
pd.merge(df8, df9, on="name", suffixes=["_L", "_R"])
```

**Output:**
```
  name  rank_L  rank_R
0   Bob       1       3
1  Jake       2       1
2  Lisa       3       4
3   Sue       4       2
```

### Aggregating and Grouping Data

Grouping and aggregating data is a common task in data analysis. The `groupby()` method is used to split the data into groups based on some criteria, and then you can apply an aggregate function (like `sum`, `mean`, `count`, etc.) to each group.

**Example: Grouping by city and calculating the mean salary**

```python
# Group by the 'City' column and calculate the mean of the other columns
city_groups = df.groupby('City').mean()
print(city_groups)
```

## Indexing and Selecting Data

Pandas provides powerful methods for indexing and selecting data from a DataFrame. The primary methods are `loc` for label-based indexing and `iloc` for integer-based indexing.

Let's use the following DataFrame for the examples in this section:

```python
import pandas as pd

data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'David'],
    'Age': [25, 30, 35, 40],
    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston']
}
df = pd.DataFrame(data)
```

### `loc` and `iloc`

- **`loc`**: Access a group of rows and columns by label(s) or a boolean array.
- **`iloc`**: Purely integer-location based indexing for selection by position.

**Example: Selecting a single row with `loc`**

```python
# Select the row with index 1
print(df.loc[1])
```

**Output:**

```
Name              Bob
Age                30
City    Los Angeles
Name: 1, dtype: object
```

**Example: Selecting a single row with `iloc`**

```python
# Select the second row (index 1)
print(df.iloc[1])
```

**Output:**

```
Name              Bob
Age                30
City    Los Angeles
Name: 1, dtype: object
```

**Example: Selecting multiple rows and a single column**

```python
# Select the 'Name' column for the first two rows
print(df.loc[0:1, 'Name'])
```

**Output:**

```
0    Alice
1      Bob
Name: Name, dtype: object
```

### Conditional Selection

You can also select data based on a condition. This is a very common and powerful technique.

**Example: Selecting rows where Age is greater than 30**

```python
# Select rows where the 'Age' column is greater than 30
print(df[df['Age'] > 30])
```

**Output:**

```
      Name  Age     City
2  Charlie   35  Chicago
3    David   40  Houston
```

## Reading and Writing Data

Pandas provides a rich set of functions for reading data from various file formats and writing data back to them. The most common formats include CSV, Excel, JSON, and SQL databases.

**Example: Reading a CSV file**

Let's assume you have a CSV file named `data.csv` with the following content:

```csv
Name,Age,City
Alice,25,New York
Bob,30,Los Angeles
Charlie,35,Chicago
David,40,Houston
```

You can read this file into a DataFrame using `pd.read_csv()`:

```python
import pandas as pd

df = pd.read_csv('data.csv')
print(df)
```

**Example: Writing to a CSV file**

You can write a DataFrame to a CSV file using the `to_csv()` method.

```python
df.to_csv('output.csv', index=False)
```

The `index=False` argument prevents pandas from writing the DataFrame index as a column in the CSV file.

## Date/Time Manipulation

Pandas has a rich set of tools for working with dates and times, which is essential for time series analysis.

### Time Series Basics

Pandas represents dates and times using the `datetime64` dtype. The `to_datetime()` function can be used to convert strings or other formats to `datetime` objects.

**Example: Creating a date range**

```python
# Create a range of dates
dates = pd.to_datetime(['2023-01-01', '2023-01-02', '2023-01-03'])

# Create a DataFrame with a time series index
ts = pd.Series(np.random.randn(len(dates)), index=dates)
print(ts)
```

**Example: Accessing date components**

Once you have a `datetime` object, you can access its components like year, month, and day.

```python
print(ts.index.year)
print(ts.index.month)
print(ts.index.day)
```
